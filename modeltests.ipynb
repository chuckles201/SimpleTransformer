{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1368b597-cb6a-4665-a9d8-27b179fafd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKeeping gradients for all nodes so we can derive expressions by scratch!\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "'''\n",
    "Doing tests in jupyter before I export them to code, so i can play around, explore,\n",
    "and think things out!\n",
    "\n",
    "\n",
    "Have fun, explain things out like your a teacher!\n",
    "'''\n",
    "\n",
    "'''\n",
    "Keeping gradients for all nodes so we can derive expressions by scratch!\n",
    "'''\n",
    "\n",
    "\n",
    "# To-DO\n",
    "# / load/save parameters, hyperparameters from hugging face and own model!\n",
    "# / Remove parameters/autograd\n",
    "# / Fuse multihead attention w/ 4d Tensors\n",
    "# / \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5204e44f-53c6-4717-851f-d0ad86d9382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing! --> GPT-4 tokenizer copy\n",
    "import tokenization\n",
    "import regex # dependency\n",
    "\n",
    "# training tokenizer\n",
    "with open('tinystories.txt', 'r',encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# fix your terrible tokenizer!####### for now we have to go for word-by-word basis :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66137553-3c2d-428c-b273-ef717132aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1\n",
    "split_ratio = 0.9\n",
    "context_length = 400\n",
    "n_embd = 360\n",
    "batch_size = 24 # testing\n",
    "n_heads = 6\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfce1f8-5398-4805-9d6c-bc0818a4d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262d5f94-3ba0-456d-8178-270ffd75dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "getting batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[57, 69],\n",
       "         [ 1, 79],\n",
       "         [76, 64],\n",
       "         [ 1, 50],\n",
       "         [71,  1],\n",
       "         [65, 70],\n",
       "         [69,  1],\n",
       "         [28, 83],\n",
       "         [64, 71],\n",
       "         [64, 61],\n",
       "         [ 1, 61],\n",
       "         [76, 61],\n",
       "         [ 1, 76],\n",
       "         [70, 60],\n",
       "         [74,  1],\n",
       "         [79, 71],\n",
       "         [76, 83],\n",
       "         [ 1, 64],\n",
       "         [ 1, 75],\n",
       "         [ 1, 64]], device='cuda:0'),\n",
       " tensor([[69, 57],\n",
       "         [79, 57],\n",
       "         [64, 57],\n",
       "         [50, 64],\n",
       "         [ 1, 72],\n",
       "         [70, 63],\n",
       "         [ 1, 57],\n",
       "         [83, 61],\n",
       "         [71,  1],\n",
       "         [61,  1],\n",
       "         [61, 78],\n",
       "         [61, 60],\n",
       "         [76, 64],\n",
       "         [60,  1],\n",
       "         [ 1, 77],\n",
       "         [71, 77],\n",
       "         [83, 29],\n",
       "         [64, 57],\n",
       "         [75, 57],\n",
       "         [64, 65]], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating data, see textdata.py\n",
    "chars = sorted(list(set(text))) # list of all characters possible, use encode/decode for them\n",
    "vocab_size= len(chars) # num of dims we should have\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)} # converting strings to ints\n",
    "itos = {i:s for i,s in enumerate(chars)} # back\n",
    "\n",
    "encode = lambda p: [stoi[s] for s in p]\n",
    "encoded_txt = encode(text)\n",
    "\n",
    "x_train = encoded_txt\n",
    "y_train = encoded_txt[1:]\n",
    "\n",
    "x_train = torch.tensor(x_train,device=device)\n",
    "y_train = torch.tensor(y_train,device=device)\n",
    "\n",
    "# need x_train, y_train\n",
    "# re-write evaluation pipleline to be actually good!\n",
    "\n",
    "print(\"getting batch\")\n",
    "def get_batch(b,t): # get batch from x!\n",
    "    xid = torch.randint(len(x_train)-t-1,size=(b,),device=device) # getting random batches!\n",
    "    batchx = torch.cat([x_train[xid[i]:xid[i]+t] for i in range(b)]).view(b,t)\n",
    "    batchy = torch.cat([y_train[xid[i]:xid[i]+t] for i in range(b)]).view(b,t)\n",
    "    return batchx,batchy\n",
    "\n",
    "get_batch(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf8f8e0-e410-43dc-9198-60f6fad68893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*|2Mâ€ŠlÃ©sgWv4Â´Câ€”z(ZCâ€“rrAtIdQ.#qzMyRi8,-Zâ€œwÂ­ðŸŽ“Wâ€™>!EAg#ZTSDb->ekWOZh$oâ€‹â€œ-I<7P:ðŸŽ“eN |&lKðŸŽ“m*!/\n",
      "kWâ€‹ðŸŽ“!Â­Kn4Â´F"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "To-Do:\n",
    "- multihead attention (parallelism?)\n",
    "- residual connections and gradients\n",
    "- layer norm and gradients\n",
    "- dropout and gradients\n",
    "- optimizing model overall? / seeing how compares to torch\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Backprop - Each block/layer has it's own backpropogation. We'll build upon these, and have lower layers\n",
    "'pass their gradients through'.\n",
    "\n",
    "- Gradient only taken when .backprop called\n",
    "- each layer has a backward function which computes the gradients for its parameters, returns\n",
    "the parameters it got fed\n",
    "- each layer takes in argument of gradient its expecting (output), and feeds to next\n",
    "layer the gradient of what it received (input)\n",
    "- Store gradients in self.grads in same order of params (and same #)\n",
    "\n",
    "-----------------------------------------------------------\n",
    "- Break up layers into operations: try to get equations!\n",
    "- Stick to chain rule; make sure at end your using things that we already have!\n",
    "\n",
    "- Check gradients with pytorch on the side!\n",
    "'''\n",
    "# delete nn.parameter when done checking grads\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "class Dropout: # setting specified percent of parameters to zero!\n",
    "    def __init__(self, dropout_rate=dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.parameters = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        mask = torch.randint(100,size=(x.shape),device=device)\n",
    "        self.mask = mask > self.dropout_rate * 100 # false for 20%\n",
    "        return x * self.mask\n",
    "        \n",
    "    def backward(self,dx):\n",
    "        return dx*self.mask # gradient of input is just masked !\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "class SelfAttention: # single head of self attention\n",
    "    def __init__(self,nin,head_size):\n",
    "        self.head_size = head_size\n",
    "        self.keys = nn.Parameter(torch.randn(nin,head_size,device=device,requires_grad=True)/ (head_size ** 0.5))\n",
    "        self.querys = nn.Parameter(torch.randn(nin,head_size,device=device,requires_grad=True)/ (head_size ** 0.5))\n",
    "        self.values = nn.Parameter(torch.randn(nin,head_size,device=device,requires_grad=True)/ (head_size ** 0.5))\n",
    "        self.parameters = [self.keys] + [self.querys] + [self.values]\n",
    "        self.grads = [None,None,None]\n",
    "        self.soft = Softmax1d()\n",
    "        self.drop = Dropout()\n",
    "        \n",
    "    def __call__(self,x,y=None): # expects B,T,n_embd(nin)\n",
    "        # saving for derivs\n",
    "        self.embdin = x\n",
    "        self.k = x @ self.keys \n",
    "        self.q = x @ self.querys\n",
    "        self.v = x @ self.values\n",
    "        \n",
    "        # self-attention! tokens communicating information\n",
    "        wei  = self.q @ torch.transpose(self.k,-2,-1) / self.head_size**0.5 # queries 'talk' to previous keys ###\n",
    "        self.m = torch.tril(torch.ones(self.k.shape[1],self.k.shape[1],device=device))\n",
    "        wei = wei.masked_fill(self.m==0,float('-inf')) # disallowing queries to talk w/ future keys!\n",
    "        self.wei = self.soft(wei) # softmaxing values\n",
    "        self.dwei = self.drop(self.wei) # dropout --> Analyze deriv ###\n",
    "        self.out = self.dwei @ self.v\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # takes in deriv of output; see /calc\n",
    "        \n",
    "        # derivative of weights/masked layer and values (dm2 is dwei)\n",
    "        dw = dx @ torch.transpose(self.v,2,1) # B,T,C @ B,C,T = B,T,T ###\n",
    "        dv = torch.transpose(self.dwei,2,1) @ dx # B,T,T @ B,T,C = B,T,C\n",
    "\n",
    "        dm3 = self.drop.backward(dw) # accounting for dropout ###\n",
    "\n",
    "        # derivative of softmax layer\n",
    "        # BE CAREFUL! --> We must multiply times dw first as to not 'loose' gradients (they sum to zero)\n",
    "        dm2 = (( (dm3 * self.wei).sum(-1,keepdim=True) * -self.wei) + self.wei * dm3) # how affecting self, then others... ###\n",
    "        dm1 = dm2.masked_fill(self.m==0,0) # just stopping gradients --> B,T,T\n",
    "        \n",
    "        # deriving Key,Query,Value original layers and embdin\n",
    "        dk = (torch.transpose(dm1,2,1) @ self.q) / self.head_size**0.5\n",
    "        dq = (dm1 @ self.k) / self.head_size**0.5\n",
    "        dkeys = (torch.transpose(self.embdin,2,1) @ dk).sum(0) # since affect all B\n",
    "        dquerys = (torch.transpose(self.embdin,2,1) @ dq).sum(0)\n",
    "        dvalues = (torch.transpose(self.embdin,2,1) @ dv).sum(0)\n",
    "        \n",
    "        self.grads[0] = dkeys # appending to parameters\n",
    "        self.grads[1] = dquerys\n",
    "        self.grads[2] = dvalues\n",
    "\n",
    "        # deriving input (embedding)\n",
    "        dembdin = dk @ self.keys.T + dq @ self.querys.T + dv @ self.values.T\n",
    "        return dembdin # returning the input derivative!\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# compute multiple heads (w/diff Q,K,V) of attention and concatenate them\n",
    "# differen heads allow different info to be aggregated w/o info being lost in QK^T\n",
    "class MultiHeadAttention: ### adding projection\n",
    "    def __init__(self,nin,n_heads,head_length): # how many heads and how long\n",
    "        self.heads = [SelfAttention(nin,head_length) for _ in range(n_heads)]\n",
    "        self.parameters = [p for h in self.heads for p in h.parameters] \n",
    "        self.grads = [None for h in self.heads for _ in h.parameters]\n",
    "        \n",
    "        \n",
    "    def __call__(self,x,y=None):\n",
    "        self.out = torch.cat([h(x) for h in self.heads], 2)\n",
    "        return self.out \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # return normal grads... but also add embd grads!\n",
    "        # splitting dx into parts/ summing!\n",
    "        n = self.parameters[0].shape[1] # how much we should step\n",
    "        dembdin = [self.heads[i].backward(dx[:,:,i*n:(i+1)*n]) for i in range(len(self.heads))] # also stores for internal parameters\n",
    "        dembdin = sum(dembdin) # also stores for internal parameters\n",
    "\n",
    "        # updating gradients\n",
    "        for h in range(len(self.heads)):\n",
    "            for p in range(3):\n",
    "                self.grads[h*3+p] = self.heads[h].grads[p] # adding all gradients of all parameters of head back\n",
    "        return dembdin\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "Full Transformer Block with a masked multihead attention layer, a feedforward layer, and residual connections, normalization,\n",
    "and dropout.\n",
    "\n",
    "Everything will be similar dimensionality, besides the linear layer which will be 4*dmodel.\n",
    "\n",
    "Each layer will have a linear projection before its normalization and residual connection.\n",
    "'''\n",
    "class TransformerBlock:\n",
    "    def __init__(self,dmodel,n_heads): # Pre-LN Architecture\n",
    "        shape = [batch_size,context_length,dmodel]\n",
    "        self.layers = [LayerNorm(shape),MultiHeadAttention(dmodel,n_heads,dmodel//n_heads),Linear(dmodel,dmodel),Dropout(),\n",
    "                       LayerNorm(shape),Linear(dmodel,dmodel*4),RELU(),Linear(dmodel*4,dmodel),Dropout()]\n",
    "        self.parameters = [p for l in self.layers for p in l.parameters]\n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.grads = [None for _ in self.parameters]\n",
    "\n",
    "    def __call__(self,x,y=None):\n",
    "        # LayerNorm, MHA, Res., LayerNorm,FFN,Res.\n",
    "        ln1 = self.layers[0](x)\n",
    "        x = self.layers[3](self.layers[2](self.layers[1](ln1))) + x # MHA, proj, drop, residual connection \n",
    "        \n",
    "        ln2 = self.layers[4](x) # norm\n",
    "        ffw = self.layers[6](self.layers[5](ln2)) # relu\n",
    "        ffw = self.layers[7](ffw) # projection\n",
    "        x = self.layers[8](ffw) + x # dropout, residual connection\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # dx6 is deriv of input to layer 6\n",
    "        # backward through all layers in reversed order w/ res. grads\n",
    "        dx8 = self.layers[8].backward(dx) # input to 8th layer (out of 7th)\n",
    "        dx7 = self.layers[7].backward(dx8)\n",
    "        dx6 = self.layers[6].backward(dx7)\n",
    "        dx5 = self.layers[5].backward(dx6)\n",
    "        dx = self.layers[4].backward(dx5) + dx # output of Dropout affecting more!\n",
    "        dx3 = self.layers[3].backward(dx)\n",
    "        dx2 = self.layers[2].backward(dx3)\n",
    "        dx1 = self.layers[1].backward(dx2) \n",
    "        dx = self.layers[0].backward(dx1) + dx\n",
    "\n",
    "        # storing all gradients for all parameters!\n",
    "        i = 0\n",
    "        for l in range(len(self.layers)): # each layer\n",
    "            for p in range(len(self.layers[l].parameters)): # each parameter\n",
    "\n",
    "                self.grads[i] = self.layers[l].grads[p]\n",
    "                i += 1\n",
    "        \n",
    "        return dx\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "class EmbeddingDouble: # position and token embedding\n",
    "    def __init__(self,vocab_size,n_embd):\n",
    "        self.embeddings = nn.Parameter(torch.randn(vocab_size,n_embd,device=device,requires_grad=True)) # B, T, C\n",
    "        self.pos_embeddings = nn.Parameter(torch.randn(context_length,n_embd,device=device,requires_grad=True)) # T,C\n",
    "        self.parameters = [self.embeddings] + [self.pos_embeddings]\n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.s = Softmax1d()\n",
    "        self.grads = [None,None]\n",
    "        \n",
    "    def __call__(self,x,y=None):\n",
    "        # replacing x vals w/ new Channel dim\n",
    "        self.idx = x # keeping this for derivs\n",
    "        B,T = x.shape\n",
    "        C = self.embeddings.shape[1]\n",
    "        id_emb = self.embeddings[x.view(B*T)].view(B,T,C)\n",
    "        pos_emb = self.pos_embeddings[torch.arange(T,device=device)].view(1,T,C)\n",
    "        \n",
    "        return id_emb + pos_emb\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # expecting B,T,C input of dembd\n",
    "        \n",
    "        # adding dembd each time its taken from idx --> can be optimized???\n",
    "        dtable1 = torch.zeros_like(self.parameters[0]) # id emb table\n",
    "\n",
    "        for i in range(dx.shape[0]): # for each batch in dembd\n",
    "            for j in range(dx.shape[1]): # each row\n",
    "                dtable1[self.idx[i,j]] += dx[i,j] # adding to indexed gradient for emb table\n",
    "        \n",
    "        dtable2 = dx.sum(0) # just the sum of all elements gives derivative!\n",
    "        self.grads[0] = dtable1\n",
    "        self.grads[1] = dtable2\n",
    "        \n",
    "        return None # first part, so no gradient passed to inputs!\n",
    "     \n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "class LayerNorm: # standardizes statistics of all rows w/ same 2nd dimension (same batch)\n",
    "    def __init__(self,shape,eps=1e-5):\n",
    "        self.gamma =nn.Parameter(torch.ones(*shape[:2],1,device=device,)) # scaling, *unpack\n",
    "        self.beta = nn.Parameter(torch.zeros(*shape[:2],1,device=device)) # shifting parameter\n",
    "        self.parameters = [self.gamma,self.beta]\n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.grads = [None,None]\n",
    "        self.eps = 1e-5 # just for making std dev more smoothe/ prev division by zero\n",
    "\n",
    "    def __call__(self,x,y=None):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "        # normalizing each col/ each input to neurons\n",
    "        xmean = x.sum(2,keepdim=True) / C\n",
    "        xdiff = x-xmean # for reuse\n",
    "        self.xvarinv = ( ( (xdiff ** 2).sum(2,keepdim=True) / (C-1) ) + self.eps)**-0.5\n",
    "        self.xnorm = xdiff * self.xvarinv\n",
    "        self.out = self.xnorm * self.gamma[:B,:T] + self.beta[:B,:T] # only using params we need\n",
    "        return self.out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # B,T,C\n",
    "        # see /calc\n",
    "        \n",
    "        # parameters\n",
    "        dgamma = (dx * self.xnorm).sum(2,keepdim=True)\n",
    "        dbeta = dx.sum(2,keepdim=True)\n",
    "        self.grads[0] = dgamma # in order\n",
    "        self.grads[1] = dbeta\n",
    "\n",
    "        # deriving input --> taking advtantage of broadcasting!\n",
    "        n = dx.shape[2] # length of rows\n",
    "        \n",
    "        c = (self.gamma * self.xvarinv) * n**-1\n",
    "        a = dx*n\n",
    "        b = dx.sum(2,keepdim=True)\n",
    "        d=  ((self.xnorm * n) / (n-1)) * ((self.xnorm*dx).sum(2,keepdim=True))\n",
    "\n",
    "        dinput = c * (a - b - d)\n",
    "        self.dinput = dinput ###\n",
    "        return dinput\n",
    "        \n",
    "        \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Linear:\n",
    "    # simple linear layer of neurons ###Del nn.parameter when done grad.-checking\n",
    "    def __init__(self,nin,nout,bias=True,div=None):\n",
    "        div = (nin ** 0.5) if div == None else div\n",
    "        self.weights = nn.Parameter(torch.randn(nin,nout,device=device,requires_grad=True) / div) ### Relu kaiming init, other\n",
    "        self.bias = nn.Parameter(torch.randn(1,nout,device=device,requires_grad=True) / nin) if bias else None ##\n",
    "        self.parameters = [self.weights] + ([self.bias] if bias else []) \n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.grads = [None] + ([None] if bias else [])\n",
    "        \n",
    "        \n",
    "    def __call__(self,x,y=None):\n",
    "        self.xin = x # saving for grad\n",
    "        self.out = x@self.weights\n",
    "        self.out += self.bias if self.bias != None else 0\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self,dx): # expecting B,T,C\n",
    "        # see /calc for equations --> x is grad from output\n",
    "        # summing on dim=0, because batch dimensions all affected by weights\n",
    "        dweights = (torch.transpose(self.xin,2,1) @ dx).sum(dim=0) # each weight hits ith col of self.in, jth of self.out\n",
    "        self.grads[0] = dweights\n",
    "        \n",
    "        if len(self.parameters) > 1: # if there is a bias, compute deriv\n",
    "            dbias = self.out.sum(dim=2)\n",
    "            self.grads[1] = dx.sum(dim=(0,1)) # contr to all batches, all elems in each col\n",
    "\n",
    "        # return the derivative of input, (B,T,in) --> (B,T,C)\n",
    "        din = (dx @ torch.transpose(self.weights,1,0))\n",
    "        return din\n",
    "\n",
    "\n",
    "class RELU: # Change!\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        \n",
    "    def __call__(self,x,y=None): # take in any shape, get red of negative values!\n",
    "        self.mask = x<0\n",
    "        return x * self.mask # returns 0 at false, corr. el at true!\n",
    "\n",
    "        \n",
    "    @ torch.no_grad()\n",
    "    def backward(self,dx):\n",
    "        return dx * self.mask # masking where the gradients are stopped\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class CrossEntropy1d: ## make more stable?\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "        self.mt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.vt = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.soft = Softmax1d()\n",
    "    \n",
    "    def __call__(self,logits,y): # x is B*T,C\n",
    "\n",
    "        # making shapes easier to view\n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T,C)\n",
    "        y = y.view(B*T)\n",
    "        self.y = y # saving for gradient\n",
    "        \n",
    "        # getting probs from logits\n",
    "        self.probs = self.soft(logits)\n",
    "        \n",
    "        # seeing how correct probs are rated by model\n",
    "        probs2 = self.probs.clone()[torch.arange(len(self.probs),device=device),y]\n",
    "        loss = -torch.log(probs2).sum() / len(probs2) # -log(mean_prob)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def backward(self, dx): # takes in global grad. (shape of output), and computes derivs\n",
    "        # see '/calculus' for equations\n",
    "        # all gradients in order of parameters\n",
    "        self.logitgrad = self.probs\n",
    "        self.logitgrad[torch.arange(self.y.shape[0]),self.y] -= 1 # selecting all i != y\n",
    "        self.logitgrad /= self.y.shape[0] # dividing by num samples\n",
    "\n",
    "        # expected shape for next bckwd \n",
    "        self.logitgrad = self.logitgrad.view(batch_size,context_length,vocab_size)   \n",
    "        return self.logitgrad # dloss is 1 so no multplication\n",
    "\n",
    "class Softmax1d:\n",
    "    def __call__(self,logits): # shape: B*T,C\n",
    "        # exponentiating and normalizing collumns of matrix\n",
    "        max_logits,_ = torch.max(logits,dim=-1,keepdim=True) # subtracting from max to avoid overflow\n",
    "        logits = logits-max_logits\n",
    "        cts = logits.exp()\n",
    "        probs = cts / cts.sum(dim=-1,keepdim=True)  # dividing by sum to normalize rows\n",
    "        \n",
    "        return probs\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "class Model:\n",
    "    def __init__(self,vocab_size=vocab_size):\n",
    "\n",
    "        # for optimization\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Layers of model! (will also be used for .backward())\n",
    "        self.layers = [EmbeddingDouble(vocab_size,n_embd),# embedding for pos and indentity\n",
    "                       TransformerBlock(n_embd,n_heads), # full transformer pre-ln block!\n",
    "                       TransformerBlock(n_embd,n_heads),\n",
    "                       TransformerBlock(n_embd,n_heads), \n",
    "                       TransformerBlock(n_embd,n_heads),\n",
    "                       TransformerBlock(n_embd,n_heads), \n",
    "                       TransformerBlock(n_embd,n_heads),\n",
    "                       TransformerBlock(n_embd,n_heads),\n",
    "                       LayerNorm([batch_size,context_length,vocab_size]),\n",
    "                       Linear(n_embd,vocab_size,div=n_embd*3), # special divison to decrease error\n",
    "                       CrossEntropy1d()] # final head of logits\n",
    "\n",
    "    # loading from json all parameters in order\n",
    "    def load(self,file=\"weights.txt\"):\n",
    "        # loading list of tensors\n",
    "        with open(file,'r') as f:\n",
    "            params = json.load(f)\n",
    "        for l in self.layers:\n",
    "            for p in range(len(l.parameters)):\n",
    "                l.parameters[p] = torch.tensor(params[p],device=device) # should match up!\n",
    "\n",
    "    def save(self,file=\"weights.txt\"):\n",
    "        # saving list of tensors\n",
    "        params = []\n",
    "        for l in self.layers:\n",
    "            for p in range(len(l.parameters)):\n",
    "                params.append(l.parameters[p]) # should match up!\n",
    "        params = [p.tolist() for p in params]\n",
    "        with open(file,'r') as f:\n",
    "            params = json.dump(params,f)\n",
    "\n",
    "    def __call__(self,idx,y=None):\n",
    "        # go through layers, give loss(training) & probs(sampling)\n",
    "\n",
    "        # Batch, Context, Channel (embd)\n",
    "        B,T = idx.shape \n",
    "        C = n_embd # Batch, Context, Channel (embd)\n",
    "\n",
    "        # feeding prev layers' data into next\n",
    "        if y != None: # getting just loss\n",
    "            x = idx\n",
    "            for layer in self.layers:\n",
    "                x = layer(x,y) # --> B,T,C\n",
    "            loss = x\n",
    "            probs = None\n",
    "            \n",
    "        else: # getting just probabilites\n",
    "            x = idx\n",
    "            self.generate_layers = self.layers.copy() # removing cross entropy element!\n",
    "            self.generate_layers.pop(-1) # removing last part..\n",
    "            for layer in self.generate_layers:\n",
    "                x = layer(x) # --> B,T,C\n",
    "            loss = None\n",
    "            soft = Softmax1d()\n",
    "            probs = soft(x).view(B,T,vocab_size) # getting probabilities for row\n",
    "\n",
    "        return loss,probs\n",
    "\n",
    "    def generate(self,idx,max_chunks): # creates new text by running past generation through model\n",
    "        \n",
    "        for i in range(max_chunks):\n",
    "            loss,probs = self(idx) # taking context in (only last couple), no y \n",
    "            char = torch.multinomial(probs[:,-1,:], num_samples=1) # last letter that came out!\n",
    "            idx = torch.cat((idx[:,-context_length+1:],char),dim = 1) # adding to context\n",
    "            print(itos[char.item()],end=\"\") # printing out letter! ###\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backprop(self):\n",
    "        # zeroing gradients\n",
    "        for layer in self.layers:\n",
    "            for i in range(len(layer.parameters)):\n",
    "                # subtracting gradient for each parameter\n",
    "                layer.grads[i] = None\n",
    "\n",
    "        grad_current = 1 # computing derivative of all layers!\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_current = layer.backward(grad_current) # feed in grad, store\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self,lr=0.001,b1=0.8,b2=0.99,w=0.001,eps=1e-8): # lr, b1, b2, decay const,eps ###\n",
    "        # AdamW gradient optimization\n",
    "        self.steps += 1\n",
    "        self.lr = self.schedule(lr)\n",
    "        self.update_ratios = [None for l in self.layers for _ in l.parameters] # for tracking\n",
    "        self.avg_grads = [None for l in self.layers for _ in l.parameters] # for tracking\n",
    "\n",
    "        # calculating moving avgs for mt, vt\n",
    "        for layer in self.layers:\n",
    "            for i in range(len(layer.parameters)):\n",
    "                layer.mt[i].data = (layer.grads[i].data * (1-b1) + b1*layer.mt[i].data) # uncorrected version\n",
    "                layer.vt[i].data = (layer.grads[i].data**2 * (1-b2) + b2*layer.vt[i].data) \n",
    "                \n",
    "        # descending w/ AdamW and weight decay\n",
    "        z = 0\n",
    "        for layer in self.layers:\n",
    "            for i in range(len(layer.parameters)):\n",
    "                # subtracting tuned gradient for each parameter and weight decay\n",
    "                mt_corrected = layer.mt[i]/(1-b1**self.steps) # correcting for bias\n",
    "                vt_corrected = layer.vt[i]/(1-b2**self.steps)\n",
    "                layer.parameters[i].data = layer.parameters[i].data - (self.lr * (mt_corrected/(torch.sqrt(vt_corrected)+eps))) - w*layer.parameters[i].data\n",
    "                \n",
    "                self.update_ratios[z] = ((self.lr * (mt_corrected/(torch.sqrt(vt_corrected)+eps)))) / (layer.parameters[i].data + 1e-17) # no decay\n",
    "                self.avg_grads[z] = layer.grads[i] # no decay\n",
    "\n",
    "                z += 1\n",
    "                \n",
    "        self.update_ratios = [\"{:e}\".format(i.abs().mean().item()) for i in self.update_ratios] ### del eventually\n",
    "        self.avg_grads = [\"{:e}\".format(i.abs().mean().item()) for i in self.avg_grads] ### del eventually\n",
    "        \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def schedule(self,lr):\n",
    "        # automatically called in step(), takes in self.steps, outputs lr\n",
    "        #lr = lr*(0.99**self.steps)\n",
    "        return lr\n",
    "\n",
    " \n",
    "m = Model(vocab_size)\n",
    "m.generate(torch.zeros(1,1,device=device,dtype=int),100) # generating random characters!\n",
    "# Fix layer norm l/b only being able to output specific type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b801bac2-27a6-4d7d-9a0b-619b64248589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model 11404178 params\n"
     ]
    }
   ],
   "source": [
    "m = Model() # takes in, normalizes!\n",
    "x,y = get_batch(batch_size,context_length)\n",
    "\n",
    "# getting num parameters\n",
    "size = 0\n",
    "for l in m.layers:\n",
    "    for p in l.parameters:\n",
    "        size += p.view(-1).shape[0]\n",
    "print(f'Size of model {size} params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c5b872-6bf7-4efc-8d54-76dacdc33335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "None\n",
      "--------------------------------------------\n",
      "tensor(3.9382e-05, device='cuda:0')\n",
      "tensor(3.9382e-05, device='cuda:0')\n",
      "--------------------------------------------\n",
      "0 EmbeddingDouble 0  Grad: tensor(-1.3489e-05, device='cuda:0') tensor(-1.3489e-05, device='cuda:0')\n",
      "1 EmbeddingDouble 1  Grad: tensor(-1.6423e-05, device='cuda:0') tensor(-1.6423e-05, device='cuda:0')\n",
      "2 TransformerBlock 0  Grad: tensor(-8.2335e-05, device='cuda:0') tensor(-8.2335e-05, device='cuda:0')\n",
      "3 TransformerBlock 1  Grad: tensor(-0.0004, device='cuda:0') tensor(-0.0004, device='cuda:0')\n",
      "4 TransformerBlock 2  Grad: tensor(6.2373e-05, device='cuda:0') tensor(6.2373e-05, device='cuda:0')\n",
      "5 TransformerBlock 3  Grad: tensor(-7.2640e-05, device='cuda:0') tensor(-7.2640e-05, device='cuda:0')\n",
      "6 TransformerBlock 4  Grad: tensor(2.8020e-05, device='cuda:0') tensor(2.8020e-05, device='cuda:0')\n",
      "7 TransformerBlock 5  Grad: tensor(1.7153e-05, device='cuda:0') tensor(1.7153e-05, device='cuda:0')\n",
      "8 TransformerBlock 6  Grad: tensor(1.1704e-05, device='cuda:0') tensor(1.1704e-05, device='cuda:0')\n",
      "9 TransformerBlock 7  Grad: tensor(5.8824e-05, device='cuda:0') tensor(5.8824e-05, device='cuda:0')\n",
      "10 TransformerBlock 8  Grad: tensor(9.1169e-05, device='cuda:0') tensor(9.1169e-05, device='cuda:0')\n",
      "11 TransformerBlock 9  Grad: tensor(-9.0133e-05, device='cuda:0') tensor(-9.0134e-05, device='cuda:0')\n",
      "12 TransformerBlock 10  Grad: tensor(-2.8929e-05, device='cuda:0') tensor(-2.8929e-05, device='cuda:0')\n",
      "13 TransformerBlock 11  Grad: tensor(-2.7395e-05, device='cuda:0') tensor(-2.7395e-05, device='cuda:0')\n",
      "14 TransformerBlock 12  Grad: tensor(-1.1472e-05, device='cuda:0') tensor(-1.1472e-05, device='cuda:0')\n",
      "15 TransformerBlock 13  Grad: tensor(6.1488e-05, device='cuda:0') tensor(6.1488e-05, device='cuda:0')\n",
      "16 TransformerBlock 14  Grad: tensor(5.0673e-05, device='cuda:0') tensor(5.0673e-05, device='cuda:0')\n",
      "17 TransformerBlock 15  Grad: tensor(-3.4520e-05, device='cuda:0') tensor(-3.4520e-05, device='cuda:0')\n",
      "18 TransformerBlock 16  Grad: tensor(-7.1307e-05, device='cuda:0') tensor(-7.1307e-05, device='cuda:0')\n",
      "19 TransformerBlock 17  Grad: tensor(3.7760e-05, device='cuda:0') tensor(3.7760e-05, device='cuda:0')\n",
      "20 TransformerBlock 18  Grad: tensor(-0.0001, device='cuda:0') tensor(-0.0001, device='cuda:0')\n",
      "21 TransformerBlock 19  Grad: tensor(-2.1433e-05, device='cuda:0') tensor(-2.1433e-05, device='cuda:0')\n",
      "22 TransformerBlock 20  Grad: tensor(3.1165e-05, device='cuda:0') tensor(3.1164e-05, device='cuda:0')\n",
      "23 TransformerBlock 21  Grad: tensor(-0.0002, device='cuda:0') tensor(-0.0002, device='cuda:0')\n",
      "24 TransformerBlock 22  Grad: tensor(1.6860e-05, device='cuda:0') tensor(1.6860e-05, device='cuda:0')\n",
      "25 TransformerBlock 23  Grad: tensor(-2.5440e-05, device='cuda:0') tensor(-2.5440e-05, device='cuda:0')\n",
      "26 TransformerBlock 24  Grad: tensor(5.1537e-05, device='cuda:0') tensor(5.1537e-05, device='cuda:0')\n",
      "27 TransformerBlock 25  Grad: tensor(-1.4255e-05, device='cuda:0') tensor(-1.4255e-05, device='cuda:0')\n",
      "28 TransformerBlock 26  Grad: tensor(8.2610e-05, device='cuda:0') tensor(8.2610e-05, device='cuda:0')\n",
      "29 TransformerBlock 27  Grad: tensor(-0.0002, device='cuda:0') tensor(-0.0002, device='cuda:0')\n",
      "30 TransformerBlock 0  Grad: tensor(-3.7846e-05, device='cuda:0') tensor(-3.7846e-05, device='cuda:0')\n",
      "31 TransformerBlock 1  Grad: tensor(6.3025e-05, device='cuda:0') tensor(6.3025e-05, device='cuda:0')\n",
      "32 TransformerBlock 2  Grad: tensor(1.0998e-05, device='cuda:0') tensor(1.0998e-05, device='cuda:0')\n",
      "33 TransformerBlock 3  Grad: tensor(1.7817e-05, device='cuda:0') tensor(1.7817e-05, device='cuda:0')\n",
      "34 TransformerBlock 4  Grad: tensor(9.3712e-06, device='cuda:0') tensor(9.3711e-06, device='cuda:0')\n",
      "35 TransformerBlock 5  Grad: tensor(-2.2440e-05, device='cuda:0') tensor(-2.2440e-05, device='cuda:0')\n",
      "36 TransformerBlock 6  Grad: tensor(1.5954e-05, device='cuda:0') tensor(1.5954e-05, device='cuda:0')\n",
      "37 TransformerBlock 7  Grad: tensor(-2.5924e-06, device='cuda:0') tensor(-2.5925e-06, device='cuda:0')\n",
      "38 TransformerBlock 8  Grad: tensor(-1.1728e-05, device='cuda:0') tensor(-1.1728e-05, device='cuda:0')\n",
      "39 TransformerBlock 9  Grad: tensor(1.4890e-05, device='cuda:0') tensor(1.4890e-05, device='cuda:0')\n",
      "40 TransformerBlock 10  Grad: tensor(2.9462e-05, device='cuda:0') tensor(2.9462e-05, device='cuda:0')\n",
      "41 TransformerBlock 11  Grad: tensor(1.8200e-05, device='cuda:0') tensor(1.8200e-05, device='cuda:0')\n",
      "42 TransformerBlock 12  Grad: tensor(-1.1588e-06, device='cuda:0') tensor(-1.1587e-06, device='cuda:0')\n",
      "43 TransformerBlock 13  Grad: tensor(4.1184e-07, device='cuda:0') tensor(4.1183e-07, device='cuda:0')\n",
      "44 TransformerBlock 14  Grad: tensor(4.4170e-05, device='cuda:0') tensor(4.4170e-05, device='cuda:0')\n",
      "45 TransformerBlock 15  Grad: tensor(2.0794e-05, device='cuda:0') tensor(2.0794e-05, device='cuda:0')\n",
      "46 TransformerBlock 16  Grad: tensor(2.2428e-06, device='cuda:0') tensor(2.2428e-06, device='cuda:0')\n",
      "47 TransformerBlock 17  Grad: tensor(-4.3515e-05, device='cuda:0') tensor(-4.3515e-05, device='cuda:0')\n",
      "48 TransformerBlock 18  Grad: tensor(-5.1372e-06, device='cuda:0') tensor(-5.1371e-06, device='cuda:0')\n",
      "49 TransformerBlock 19  Grad: tensor(3.8225e-05, device='cuda:0') tensor(3.8225e-05, device='cuda:0')\n",
      "50 TransformerBlock 20  Grad: tensor(0.0001, device='cuda:0') tensor(0.0001, device='cuda:0')\n",
      "51 TransformerBlock 21  Grad: tensor(-4.2169e-05, device='cuda:0') tensor(-4.2169e-05, device='cuda:0')\n",
      "52 TransformerBlock 22  Grad: tensor(2.3514e-06, device='cuda:0') tensor(2.3514e-06, device='cuda:0')\n",
      "53 TransformerBlock 23  Grad: tensor(5.8155e-06, device='cuda:0') tensor(5.8155e-06, device='cuda:0')\n",
      "54 TransformerBlock 24  Grad: tensor(-7.3015e-06, device='cuda:0') tensor(-7.3015e-06, device='cuda:0')\n",
      "55 TransformerBlock 25  Grad: tensor(1.9268e-05, device='cuda:0') tensor(1.9268e-05, device='cuda:0')\n",
      "56 TransformerBlock 26  Grad: tensor(2.5144e-05, device='cuda:0') tensor(2.5144e-05, device='cuda:0')\n",
      "57 TransformerBlock 27  Grad: tensor(-3.0903e-05, device='cuda:0') tensor(-3.0903e-05, device='cuda:0')\n",
      "58 TransformerBlock 0  Grad: tensor(1.6245e-05, device='cuda:0') tensor(1.6245e-05, device='cuda:0')\n",
      "59 TransformerBlock 1  Grad: tensor(1.5670e-06, device='cuda:0') tensor(1.5671e-06, device='cuda:0')\n",
      "60 TransformerBlock 2  Grad: tensor(-1.2474e-05, device='cuda:0') tensor(-1.2474e-05, device='cuda:0')\n",
      "61 TransformerBlock 3  Grad: tensor(-1.1735e-05, device='cuda:0') tensor(-1.1735e-05, device='cuda:0')\n",
      "62 TransformerBlock 4  Grad: tensor(9.9749e-06, device='cuda:0') tensor(9.9749e-06, device='cuda:0')\n",
      "63 TransformerBlock 5  Grad: tensor(5.5395e-06, device='cuda:0') tensor(5.5396e-06, device='cuda:0')\n",
      "64 TransformerBlock 6  Grad: tensor(-3.6881e-05, device='cuda:0') tensor(-3.6881e-05, device='cuda:0')\n",
      "65 TransformerBlock 7  Grad: tensor(-4.3739e-06, device='cuda:0') tensor(-4.3739e-06, device='cuda:0')\n",
      "66 TransformerBlock 8  Grad: tensor(-2.6956e-05, device='cuda:0') tensor(-2.6956e-05, device='cuda:0')\n",
      "67 TransformerBlock 9  Grad: tensor(-3.4843e-05, device='cuda:0') tensor(-3.4843e-05, device='cuda:0')\n",
      "68 TransformerBlock 10  Grad: tensor(1.1827e-05, device='cuda:0') tensor(1.1827e-05, device='cuda:0')\n",
      "69 TransformerBlock 11  Grad: tensor(-3.1004e-05, device='cuda:0') tensor(-3.1004e-05, device='cuda:0')\n",
      "70 TransformerBlock 12  Grad: tensor(-3.5415e-05, device='cuda:0') tensor(-3.5415e-05, device='cuda:0')\n",
      "71 TransformerBlock 13  Grad: tensor(2.7655e-06, device='cuda:0') tensor(2.7655e-06, device='cuda:0')\n",
      "72 TransformerBlock 14  Grad: tensor(2.3039e-05, device='cuda:0') tensor(2.3039e-05, device='cuda:0')\n",
      "73 TransformerBlock 15  Grad: tensor(-4.2345e-07, device='cuda:0') tensor(-4.2342e-07, device='cuda:0')\n",
      "74 TransformerBlock 16  Grad: tensor(2.8923e-05, device='cuda:0') tensor(2.8923e-05, device='cuda:0')\n",
      "75 TransformerBlock 17  Grad: tensor(-1.6577e-05, device='cuda:0') tensor(-1.6577e-05, device='cuda:0')\n",
      "76 TransformerBlock 18  Grad: tensor(-2.6870e-05, device='cuda:0') tensor(-2.6870e-05, device='cuda:0')\n",
      "77 TransformerBlock 19  Grad: tensor(-5.5037e-06, device='cuda:0') tensor(-5.5037e-06, device='cuda:0')\n",
      "78 TransformerBlock 20  Grad: tensor(-8.6542e-06, device='cuda:0') tensor(-8.6541e-06, device='cuda:0')\n",
      "79 TransformerBlock 21  Grad: tensor(5.8837e-06, device='cuda:0') tensor(5.8837e-06, device='cuda:0')\n",
      "80 TransformerBlock 22  Grad: tensor(-7.2878e-06, device='cuda:0') tensor(-7.2878e-06, device='cuda:0')\n",
      "81 TransformerBlock 23  Grad: tensor(-3.4432e-06, device='cuda:0') tensor(-3.4432e-06, device='cuda:0')\n",
      "82 TransformerBlock 24  Grad: tensor(1.4477e-06, device='cuda:0') tensor(1.4477e-06, device='cuda:0')\n",
      "83 TransformerBlock 25  Grad: tensor(3.7126e-06, device='cuda:0') tensor(3.7126e-06, device='cuda:0')\n",
      "84 TransformerBlock 26  Grad: tensor(-1.4959e-06, device='cuda:0') tensor(-1.4959e-06, device='cuda:0')\n",
      "85 TransformerBlock 27  Grad: tensor(-5.1346e-06, device='cuda:0') tensor(-5.1346e-06, device='cuda:0')\n",
      "86 TransformerBlock 0  Grad: tensor(-1.2778e-05, device='cuda:0') tensor(-1.2778e-05, device='cuda:0')\n",
      "87 TransformerBlock 1  Grad: tensor(-1.1086e-05, device='cuda:0') tensor(-1.1086e-05, device='cuda:0')\n",
      "88 TransformerBlock 2  Grad: tensor(3.0496e-05, device='cuda:0') tensor(3.0496e-05, device='cuda:0')\n",
      "89 TransformerBlock 3  Grad: tensor(1.6311e-05, device='cuda:0') tensor(1.6311e-05, device='cuda:0')\n",
      "90 TransformerBlock 4  Grad: tensor(5.3574e-06, device='cuda:0') tensor(5.3574e-06, device='cuda:0')\n",
      "91 TransformerBlock 5  Grad: tensor(-8.8595e-07, device='cuda:0') tensor(-8.8596e-07, device='cuda:0')\n",
      "92 TransformerBlock 6  Grad: tensor(1.3408e-06, device='cuda:0') tensor(1.3408e-06, device='cuda:0')\n",
      "93 TransformerBlock 7  Grad: tensor(-3.6238e-06, device='cuda:0') tensor(-3.6238e-06, device='cuda:0')\n",
      "94 TransformerBlock 8  Grad: tensor(-7.7793e-06, device='cuda:0') tensor(-7.7793e-06, device='cuda:0')\n",
      "95 TransformerBlock 9  Grad: tensor(5.4808e-06, device='cuda:0') tensor(5.4808e-06, device='cuda:0')\n",
      "96 TransformerBlock 10  Grad: tensor(3.5381e-06, device='cuda:0') tensor(3.5381e-06, device='cuda:0')\n",
      "97 TransformerBlock 11  Grad: tensor(9.1603e-06, device='cuda:0') tensor(9.1603e-06, device='cuda:0')\n",
      "98 TransformerBlock 12  Grad: tensor(5.4266e-06, device='cuda:0') tensor(5.4266e-06, device='cuda:0')\n",
      "99 TransformerBlock 13  Grad: tensor(1.8711e-05, device='cuda:0') tensor(1.8711e-05, device='cuda:0')\n",
      "100 TransformerBlock 14  Grad: tensor(-9.8252e-06, device='cuda:0') tensor(-9.8252e-06, device='cuda:0')\n",
      "101 TransformerBlock 15  Grad: tensor(1.6796e-05, device='cuda:0') tensor(1.6796e-05, device='cuda:0')\n",
      "102 TransformerBlock 16  Grad: tensor(-1.2295e-06, device='cuda:0') tensor(-1.2296e-06, device='cuda:0')\n",
      "103 TransformerBlock 17  Grad: tensor(8.4872e-06, device='cuda:0') tensor(8.4872e-06, device='cuda:0')\n",
      "104 TransformerBlock 18  Grad: tensor(-3.0876e-06, device='cuda:0') tensor(-3.0876e-06, device='cuda:0')\n",
      "105 TransformerBlock 19  Grad: tensor(9.2783e-06, device='cuda:0') tensor(9.2783e-06, device='cuda:0')\n",
      "106 TransformerBlock 20  Grad: tensor(-1.5034e-05, device='cuda:0') tensor(-1.5034e-05, device='cuda:0')\n",
      "107 TransformerBlock 21  Grad: tensor(-2.2039e-05, device='cuda:0') tensor(-2.2039e-05, device='cuda:0')\n",
      "108 TransformerBlock 22  Grad: tensor(8.2966e-08, device='cuda:0') tensor(8.2965e-08, device='cuda:0')\n",
      "109 TransformerBlock 23  Grad: tensor(-1.7970e-06, device='cuda:0') tensor(-1.7970e-06, device='cuda:0')\n",
      "110 TransformerBlock 24  Grad: tensor(-6.5181e-06, device='cuda:0') tensor(-6.5181e-06, device='cuda:0')\n",
      "111 TransformerBlock 25  Grad: tensor(2.5610e-05, device='cuda:0') tensor(2.5610e-05, device='cuda:0')\n",
      "112 TransformerBlock 26  Grad: tensor(5.3147e-06, device='cuda:0') tensor(5.3147e-06, device='cuda:0')\n",
      "113 TransformerBlock 27  Grad: tensor(-1.1713e-05, device='cuda:0') tensor(-1.1713e-05, device='cuda:0')\n",
      "114 TransformerBlock 0  Grad: tensor(8.7688e-06, device='cuda:0') tensor(8.7688e-06, device='cuda:0')\n",
      "115 TransformerBlock 1  Grad: tensor(5.7773e-07, device='cuda:0') tensor(5.7772e-07, device='cuda:0')\n",
      "116 TransformerBlock 2  Grad: tensor(-7.8227e-07, device='cuda:0') tensor(-7.8226e-07, device='cuda:0')\n",
      "117 TransformerBlock 3  Grad: tensor(-1.6730e-06, device='cuda:0') tensor(-1.6730e-06, device='cuda:0')\n",
      "118 TransformerBlock 4  Grad: tensor(-1.9955e-05, device='cuda:0') tensor(-1.9955e-05, device='cuda:0')\n",
      "119 TransformerBlock 5  Grad: tensor(-2.1798e-06, device='cuda:0') tensor(-2.1798e-06, device='cuda:0')\n",
      "120 TransformerBlock 6  Grad: tensor(-3.5129e-06, device='cuda:0') tensor(-3.5129e-06, device='cuda:0')\n",
      "121 TransformerBlock 7  Grad: tensor(-5.9831e-06, device='cuda:0') tensor(-5.9831e-06, device='cuda:0')\n",
      "122 TransformerBlock 8  Grad: tensor(-7.8415e-07, device='cuda:0') tensor(-7.8414e-07, device='cuda:0')\n",
      "123 TransformerBlock 9  Grad: tensor(1.1695e-05, device='cuda:0') tensor(1.1695e-05, device='cuda:0')\n",
      "124 TransformerBlock 10  Grad: tensor(5.5397e-06, device='cuda:0') tensor(5.5397e-06, device='cuda:0')\n",
      "125 TransformerBlock 11  Grad: tensor(4.3416e-06, device='cuda:0') tensor(4.3416e-06, device='cuda:0')\n",
      "126 TransformerBlock 12  Grad: tensor(-1.6188e-06, device='cuda:0') tensor(-1.6188e-06, device='cuda:0')\n",
      "127 TransformerBlock 13  Grad: tensor(-7.5083e-07, device='cuda:0') tensor(-7.5082e-07, device='cuda:0')\n",
      "128 TransformerBlock 14  Grad: tensor(1.7143e-05, device='cuda:0') tensor(1.7143e-05, device='cuda:0')\n",
      "129 TransformerBlock 15  Grad: tensor(5.3645e-07, device='cuda:0') tensor(5.3645e-07, device='cuda:0')\n",
      "130 TransformerBlock 16  Grad: tensor(4.5770e-06, device='cuda:0') tensor(4.5770e-06, device='cuda:0')\n",
      "131 TransformerBlock 17  Grad: tensor(-1.6412e-05, device='cuda:0') tensor(-1.6412e-05, device='cuda:0')\n",
      "132 TransformerBlock 18  Grad: tensor(-1.0422e-05, device='cuda:0') tensor(-1.0422e-05, device='cuda:0')\n",
      "133 TransformerBlock 19  Grad: tensor(9.5870e-06, device='cuda:0') tensor(9.5870e-06, device='cuda:0')\n",
      "134 TransformerBlock 20  Grad: tensor(-3.5180e-06, device='cuda:0') tensor(-3.5180e-06, device='cuda:0')\n",
      "135 TransformerBlock 21  Grad: tensor(1.7301e-05, device='cuda:0') tensor(1.7301e-05, device='cuda:0')\n",
      "136 TransformerBlock 22  Grad: tensor(-6.7676e-07, device='cuda:0') tensor(-6.7676e-07, device='cuda:0')\n",
      "137 TransformerBlock 23  Grad: tensor(9.8185e-08, device='cuda:0') tensor(9.8184e-08, device='cuda:0')\n",
      "138 TransformerBlock 24  Grad: tensor(-1.9133e-06, device='cuda:0') tensor(-1.9133e-06, device='cuda:0')\n",
      "139 TransformerBlock 25  Grad: tensor(-1.1960e-08, device='cuda:0') tensor(-1.1961e-08, device='cuda:0')\n",
      "140 TransformerBlock 26  Grad: tensor(-6.4322e-06, device='cuda:0') tensor(-6.4322e-06, device='cuda:0')\n",
      "141 TransformerBlock 27  Grad: tensor(1.4678e-05, device='cuda:0') tensor(1.4678e-05, device='cuda:0')\n",
      "142 TransformerBlock 0  Grad: tensor(-2.6666e-06, device='cuda:0') tensor(-2.6666e-06, device='cuda:0')\n",
      "143 TransformerBlock 1  Grad: tensor(-1.8681e-06, device='cuda:0') tensor(-1.8681e-06, device='cuda:0')\n",
      "144 TransformerBlock 2  Grad: tensor(-2.5319e-06, device='cuda:0') tensor(-2.5319e-06, device='cuda:0')\n",
      "145 TransformerBlock 3  Grad: tensor(-3.0537e-06, device='cuda:0') tensor(-3.0537e-06, device='cuda:0')\n",
      "146 TransformerBlock 4  Grad: tensor(1.1083e-06, device='cuda:0') tensor(1.1083e-06, device='cuda:0')\n",
      "147 TransformerBlock 5  Grad: tensor(-4.1423e-06, device='cuda:0') tensor(-4.1423e-06, device='cuda:0')\n",
      "148 TransformerBlock 6  Grad: tensor(-4.3288e-07, device='cuda:0') tensor(-4.3289e-07, device='cuda:0')\n",
      "149 TransformerBlock 7  Grad: tensor(1.1498e-07, device='cuda:0') tensor(1.1498e-07, device='cuda:0')\n",
      "150 TransformerBlock 8  Grad: tensor(7.2217e-07, device='cuda:0') tensor(7.2217e-07, device='cuda:0')\n",
      "151 TransformerBlock 9  Grad: tensor(-8.9647e-06, device='cuda:0') tensor(-8.9647e-06, device='cuda:0')\n",
      "152 TransformerBlock 10  Grad: tensor(-4.2115e-07, device='cuda:0') tensor(-4.2115e-07, device='cuda:0')\n",
      "153 TransformerBlock 11  Grad: tensor(-2.8896e-06, device='cuda:0') tensor(-2.8896e-06, device='cuda:0')\n",
      "154 TransformerBlock 12  Grad: tensor(-9.0566e-07, device='cuda:0') tensor(-9.0566e-07, device='cuda:0')\n",
      "155 TransformerBlock 13  Grad: tensor(6.9397e-06, device='cuda:0') tensor(6.9397e-06, device='cuda:0')\n",
      "156 TransformerBlock 14  Grad: tensor(7.5692e-07, device='cuda:0') tensor(7.5692e-07, device='cuda:0')\n",
      "157 TransformerBlock 15  Grad: tensor(-1.1892e-05, device='cuda:0') tensor(-1.1892e-05, device='cuda:0')\n",
      "158 TransformerBlock 16  Grad: tensor(2.5778e-06, device='cuda:0') tensor(2.5778e-06, device='cuda:0')\n",
      "159 TransformerBlock 17  Grad: tensor(-2.8496e-06, device='cuda:0') tensor(-2.8496e-06, device='cuda:0')\n",
      "160 TransformerBlock 18  Grad: tensor(3.4617e-06, device='cuda:0') tensor(3.4617e-06, device='cuda:0')\n",
      "161 TransformerBlock 19  Grad: tensor(-3.1815e-06, device='cuda:0') tensor(-3.1815e-06, device='cuda:0')\n",
      "162 TransformerBlock 20  Grad: tensor(5.1749e-06, device='cuda:0') tensor(5.1749e-06, device='cuda:0')\n",
      "163 TransformerBlock 21  Grad: tensor(2.5335e-05, device='cuda:0') tensor(2.5335e-05, device='cuda:0')\n",
      "164 TransformerBlock 22  Grad: tensor(-3.2099e-07, device='cuda:0') tensor(-3.2099e-07, device='cuda:0')\n",
      "165 TransformerBlock 23  Grad: tensor(5.3816e-07, device='cuda:0') tensor(5.3816e-07, device='cuda:0')\n",
      "166 TransformerBlock 24  Grad: tensor(2.0269e-06, device='cuda:0') tensor(2.0269e-06, device='cuda:0')\n",
      "167 TransformerBlock 25  Grad: tensor(-2.1082e-07, device='cuda:0') tensor(-2.1082e-07, device='cuda:0')\n",
      "168 TransformerBlock 26  Grad: tensor(-4.6805e-06, device='cuda:0') tensor(-4.6805e-06, device='cuda:0')\n",
      "169 TransformerBlock 27  Grad: tensor(2.3532e-05, device='cuda:0') tensor(2.3532e-05, device='cuda:0')\n",
      "170 TransformerBlock 0  Grad: tensor(1.3717e-05, device='cuda:0') tensor(1.3717e-05, device='cuda:0')\n",
      "171 TransformerBlock 1  Grad: tensor(-2.3807e-06, device='cuda:0') tensor(-2.3807e-06, device='cuda:0')\n",
      "172 TransformerBlock 2  Grad: tensor(-5.4718e-06, device='cuda:0') tensor(-5.4718e-06, device='cuda:0')\n",
      "173 TransformerBlock 3  Grad: tensor(1.0594e-06, device='cuda:0') tensor(1.0594e-06, device='cuda:0')\n",
      "174 TransformerBlock 4  Grad: tensor(3.9341e-06, device='cuda:0') tensor(3.9341e-06, device='cuda:0')\n",
      "175 TransformerBlock 5  Grad: tensor(-1.3179e-06, device='cuda:0') tensor(-1.3179e-06, device='cuda:0')\n",
      "176 TransformerBlock 6  Grad: tensor(1.8709e-07, device='cuda:0') tensor(1.8709e-07, device='cuda:0')\n",
      "177 TransformerBlock 7  Grad: tensor(5.3893e-06, device='cuda:0') tensor(5.3893e-06, device='cuda:0')\n",
      "178 TransformerBlock 8  Grad: tensor(-2.0894e-06, device='cuda:0') tensor(-2.0894e-06, device='cuda:0')\n",
      "179 TransformerBlock 9  Grad: tensor(1.8466e-06, device='cuda:0') tensor(1.8466e-06, device='cuda:0')\n",
      "180 TransformerBlock 10  Grad: tensor(-4.8532e-06, device='cuda:0') tensor(-4.8532e-06, device='cuda:0')\n",
      "181 TransformerBlock 11  Grad: tensor(-5.0068e-06, device='cuda:0') tensor(-5.0068e-06, device='cuda:0')\n",
      "182 TransformerBlock 12  Grad: tensor(-1.7095e-06, device='cuda:0') tensor(-1.7095e-06, device='cuda:0')\n",
      "183 TransformerBlock 13  Grad: tensor(4.3761e-06, device='cuda:0') tensor(4.3761e-06, device='cuda:0')\n",
      "184 TransformerBlock 14  Grad: tensor(5.9373e-07, device='cuda:0') tensor(5.9373e-07, device='cuda:0')\n",
      "185 TransformerBlock 15  Grad: tensor(-5.2606e-06, device='cuda:0') tensor(-5.2606e-06, device='cuda:0')\n",
      "186 TransformerBlock 16  Grad: tensor(-1.7578e-07, device='cuda:0') tensor(-1.7579e-07, device='cuda:0')\n",
      "187 TransformerBlock 17  Grad: tensor(1.0597e-05, device='cuda:0') tensor(1.0597e-05, device='cuda:0')\n",
      "188 TransformerBlock 18  Grad: tensor(2.3793e-06, device='cuda:0') tensor(2.3793e-06, device='cuda:0')\n",
      "189 TransformerBlock 19  Grad: tensor(8.9976e-06, device='cuda:0') tensor(8.9977e-06, device='cuda:0')\n",
      "190 TransformerBlock 20  Grad: tensor(6.1197e-06, device='cuda:0') tensor(6.1197e-06, device='cuda:0')\n",
      "191 TransformerBlock 21  Grad: tensor(1.1781e-05, device='cuda:0') tensor(1.1781e-05, device='cuda:0')\n",
      "192 TransformerBlock 22  Grad: tensor(7.7522e-08, device='cuda:0') tensor(7.7522e-08, device='cuda:0')\n",
      "193 TransformerBlock 23  Grad: tensor(-1.1023e-07, device='cuda:0') tensor(-1.1023e-07, device='cuda:0')\n",
      "194 TransformerBlock 24  Grad: tensor(1.1690e-06, device='cuda:0') tensor(1.1690e-06, device='cuda:0')\n",
      "195 TransformerBlock 25  Grad: tensor(2.2766e-06, device='cuda:0') tensor(2.2766e-06, device='cuda:0')\n",
      "196 TransformerBlock 26  Grad: tensor(-4.4435e-06, device='cuda:0') tensor(-4.4435e-06, device='cuda:0')\n",
      "197 TransformerBlock 27  Grad: tensor(1.5391e-05, device='cuda:0') tensor(1.5391e-05, device='cuda:0')\n",
      "198 LayerNorm 0  Grad: tensor(7.9292e-07, device='cuda:0') tensor(7.9292e-07, device='cuda:0')\n",
      "199 LayerNorm 1  Grad: tensor(1.5782e-06, device='cuda:0') tensor(1.5782e-06, device='cuda:0')\n",
      "200 Linear 0  Grad: tensor(-0.0005, device='cuda:0') tensor(-0.0005, device='cuda:0')\n",
      "201 Linear 1  Grad: tensor(0.0102, device='cuda:0') tensor(0.0102, device='cuda:0')\n",
      "['1.624134e-03', '2.263436e-03', '2.994797e-04', '1.000000e+00', '2.460957e-02', '1.811373e-02', '1.879919e-02', '2.405792e-02', '1.677464e-02', '1.802363e-02', '1.895071e-02', '1.642482e-02', '2.199081e-02', '2.302536e-02', '1.735210e-02', '1.860093e-02', '1.882841e-02', '1.828909e-02', '2.493290e-02', '1.938653e-02', '2.154688e-02', '1.521687e-02', '6.805757e-02', '6.575675e-01', '2.974664e-04', '1.000000e+00', '5.288936e-02', '8.153636e-01', '1.360906e-01', '3.188841e+00', '2.988263e-04', '1.000000e+00', '6.038227e-02', '5.416524e-02', '1.544415e-02', '1.824811e-02', '2.877479e-02', '2.145755e-02', '1.728873e-02', '4.915667e-02', '2.137598e-02', '2.348392e-02', '2.606736e-02', '3.331586e-02', '3.901004e-02', '4.973280e-02', '2.846900e-02', '2.366405e-02', '2.238000e-02', '2.316619e-02', '7.230698e-02', '3.585329e+00', '2.949602e-04', '1.000000e+00', '1.094681e-01', '7.583398e-01', '1.193271e-01', '2.085398e+00', '2.982430e-04', '1.000000e+00', '1.965802e-02', '1.868354e-02', '1.811872e-02', '2.342528e-02', '1.662878e-02', '2.999639e-02', '2.061483e-02', '1.939733e-02', '1.721329e-02', '1.731036e-02', '1.673075e-02', '2.018208e-02', '1.572444e-02', '1.768440e-02', '1.856633e-02', '6.092634e-02', '2.951263e-02', '1.544793e-02', '5.765343e-02', '8.500386e-01', '2.916072e-04', '1.000000e+00', '6.271514e-02', '6.315572e-01', '1.572541e-01', '2.258060e+00', '2.973343e-04', '1.000000e+00', '2.468004e-02', '2.267358e-02', '3.187512e-02', '1.994977e-02', '1.698158e-02', '7.699530e-02', '1.764326e-02', '2.233892e-02', '1.768685e-01', '2.862385e-02', '3.425300e-02', '2.036233e-02', '2.894852e-02', '8.177131e-02', '1.807300e-02', '1.674265e-02', '2.205218e-02', '2.090740e-02', '5.543232e-02', '7.851880e-01', '2.864957e-04', '1.000000e+00', '1.368600e-01', '2.238652e+00', '1.766891e-01', '2.649820e+00', '2.961000e-04', '1.000000e+00', '4.278063e-02', '2.111879e-02', '1.796423e-02', '1.683241e-02', '3.183595e-02', '5.631578e-02', '1.793285e-02', '1.442621e-02', '2.060860e-02', '2.072565e-02', '2.821951e-02', '4.073495e-02', '1.754360e-02', '2.340380e-02', '8.457012e-02', '2.053469e-02', '3.299278e-02', '2.844681e-02', '7.150996e-02', '5.357404e-01', '2.821500e-04', '1.000000e+00', '1.036469e-01', '8.444178e-01', '1.583345e-01', '4.460773e+00', '2.947514e-04', '1.000000e+00', '1.831913e-02', '1.986586e-02', '2.155505e-02', '1.647506e-02', '4.542185e-02', '2.093356e-02', '4.516050e-02', '2.130842e-02', '1.937069e-02', '1.970738e-02', '4.548709e-02', '2.750190e-02', '2.929962e-02', '1.386312e-02', '2.578676e-02', '3.532018e-02', '3.365186e-02', '1.815803e-02', '6.639738e-02', '7.594242e-01', '2.757685e-04', '1.000000e+00', '7.554246e-02', '4.922481e-01', '1.115113e-01', '1.350432e+00', '2.934381e-04', '1.000000e+00', '2.058451e-02', '2.312925e-02', '3.719256e-01', '3.301162e-02', '2.449928e-02', '1.811481e-02', '1.972697e-02', '2.007254e-02', '1.712703e-02', '2.867415e-02', '2.158115e-02', '1.872513e-02', '1.966736e-02', '1.872000e-02', '1.327648e-02', '3.141223e-02', '1.848534e-02', '1.504591e-02', '5.388480e-02', '1.271727e+00', '2.705131e-04', '1.000000e+00', '6.625862e-02', '4.810807e-01', '2.320511e-01', '1.774438e+00', '2.931913e-04', '1.000000e+00', '3.057398e+00', '1.011189e+00']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "loss = m(x,y)[0]\n",
    "l1 = loss.clone().sum() # 1 grad in corresp to 1s\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "x.grad= None\n",
    "for h in m.layers:\n",
    "    for p in h.parameters:\n",
    "        p.grad = None\n",
    "\n",
    "\n",
    "bw = m.backprop()\n",
    "l1.backward()\n",
    "l1.retain_grad()\n",
    "\n",
    "print(l1.grad)\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(m.layers[0].grads[0][1,2])\n",
    "print(m.layers[0].parameters[0].grad[1,2])\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "z = 0\n",
    "for layer in m.layers:\n",
    "    for i in range(len(layer.parameters)):\n",
    "        # subtracting gradient for each parameter\n",
    "        print(z,layer.__class__.__name__,i,\" Grad:\",layer.grads[i].view(-1)[10], layer.parameters[i].grad.view(-1)[10])\n",
    "        z += 1\n",
    "m.step(3e-4) # all 1st grads are fine, something happens in-between...\n",
    "print(m.update_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e908a0ca-0596-4579-9bdf-7ce0c9dea4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Model()\n",
    "\n",
    "# test data\n",
    "sti = 0\n",
    "lri = []\n",
    "stepi = []\n",
    "lossi =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da0c439-4d0f-4ca1-9ba2-c8ac8903379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training,..,.\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.376622e+00 6.867s| epoch: 0 | grad test: -4.124786e-04,-4.124786e-04\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.392840e-02'), (1, '1.582974e-01'), (2, '1.847617e-02'), (3, '4.743830e-01'), (4, '2.203156e-01'), (5, '1.630394e-01'), (6, '1.645868e-01'), (7, '1.972250e-01'), (8, '1.077953e-01'), (9, '2.098113e-01'), (10, '1.499664e-01'), (11, '1.925649e-01'), (12, '1.408016e-01'), (13, '1.442586e-01'), (14, '1.288267e-01'), (15, '2.573358e-01'), (16, '1.151638e-01'), (17, '1.664656e-01'), (18, '1.968260e-01'), (19, '1.272740e-01'), (20, '2.062461e-01'), (21, '1.747600e-01'), (22, '1.705391e-01'), (23, '2.384684e-02'), (24, '9.990384e-03'), (25, '1.833568e-01'), (26, '1.249155e-01'), (27, '5.479120e-02'), (28, '1.219777e-01'), (29, '3.989914e-02'), (30, '1.218125e-02'), (31, '3.005627e-01'), (32, '1.177345e-01'), (33, '1.536281e-01'), (34, '1.598738e-01'), (35, '1.063479e-01'), (36, '1.708207e-01'), (37, '2.096872e-01'), (38, '1.042851e-01'), (39, '1.639860e-01'), (40, '1.090469e+00'), (41, '1.342222e-01'), (42, '1.059444e-01'), (43, '1.845429e-01'), (44, '1.403757e-01'), (45, '1.002147e-01'), (46, '1.347551e-01'), (47, '1.082710e-01'), (48, '1.310549e-01'), (49, '2.579875e-01'), (50, '1.627461e-01'), (51, '4.343943e-02'), (52, '9.705402e-03'), (53, '1.791543e+00'), (54, '8.975296e-02'), (55, '1.749543e-02'), (56, '4.184678e-02'), (57, '6.144803e-02'), (58, '1.125416e-02'), (59, '4.448125e-01'), (60, '1.489683e-01'), (61, '1.094920e-01'), (62, '1.979520e-01'), (63, '6.382869e-01'), (64, '1.221793e-01'), (65, '1.617248e-01'), (66, '1.204981e-01'), (67, '3.181655e-01'), (68, '1.624157e-01'), (69, '1.071583e-01'), (70, '1.037190e-01'), (71, '1.685599e-01'), (72, '1.481115e-01'), (73, '1.861375e-01'), (74, '9.959022e-02'), (75, '1.124068e-01'), (76, '1.160150e-01'), (77, '9.540322e-02'), (78, '5.384606e-01'), (79, '8.068879e-01'), (80, '9.175857e-03'), (81, '1.578951e-01'), (82, '7.747298e-02'), (83, '8.191099e-03'), (84, '1.040080e-01'), (85, '3.948297e-02'), (86, '9.215357e-03'), (87, '2.314173e-01'), (88, '1.023588e-01'), (89, '1.207464e-01'), (90, '9.420510e-02'), (91, '1.666799e-01'), (92, '3.245012e-01'), (93, '8.714729e-02'), (94, '1.350356e-01'), (95, '1.492212e-01'), (96, '1.296144e-01'), (97, '1.322873e-01'), (98, '1.313525e-01'), (99, '1.445111e-01'), (100, '3.842441e-01'), (101, '1.304726e-01'), (102, '1.513785e-01'), (103, '1.081782e-01'), (104, '1.045238e-01'), (105, '2.097980e-01'), (106, '1.559464e-01'), (107, '5.046694e-02'), (108, '9.729777e-03'), (109, '5.143058e+00'), (110, '9.601635e-02'), (111, '1.325241e-02'), (112, '8.793655e-02'), (113, '2.145010e-01'), (114, '2.684111e-02'), (115, '2.029934e-01'), (116, '1.925728e-01'), (117, '9.958789e-02'), (118, '1.938005e-01'), (119, '1.067814e-01'), (120, '2.263119e-01'), (121, '2.646814e-01'), (122, '2.272359e-01'), (123, '1.143672e-01'), (124, '1.066013e-01'), (125, '1.247173e-01'), (126, '1.108695e-01'), (127, '1.276152e-01'), (128, '1.087662e-01'), (129, '8.350046e-02'), (130, '6.644904e-02'), (131, '1.554536e-01'), (132, '1.238829e-01'), (133, '9.454756e-02'), (134, '1.400181e-01'), (135, '6.800836e-02'), (136, '1.017160e-02'), (137, '1.634600e-01'), (138, '9.260350e-02'), (139, '1.444754e-02'), (140, '6.325159e-02'), (141, '1.002828e-01'), (142, '6.912293e-03'), (143, '1.344226e-01'), (144, '7.855655e-02'), (145, '7.574843e-02'), (146, '1.215252e-01'), (147, '8.827394e-02'), (148, '1.082497e-01'), (149, '9.233018e-02'), (150, '1.497712e-01'), (151, '9.358460e-02'), (152, '9.970053e-02'), (153, '1.451197e-01'), (154, '7.115523e-02'), (155, '8.664310e-02'), (156, '9.995292e-02'), (157, '1.039724e-01'), (158, '1.139828e-01'), (159, '9.713422e-02'), (160, '1.176782e-01'), (161, '9.284501e-02'), (162, '1.928924e-01'), (163, '6.512259e-01'), (164, '7.875727e-03'), (165, '1.640436e-01'), (166, '8.079331e-02'), (167, '5.351119e-02'), (168, '8.894671e-02'), (169, '1.407561e-01'), (170, '5.882975e-03'), (171, '8.301465e-02'), (172, '1.174417e-01'), (173, '1.474500e-01'), (174, '1.058001e-01'), (175, '6.794331e-02'), (176, '7.520673e-02'), (177, '6.828672e-02'), (178, '6.636950e-02'), (179, '7.824229e-02'), (180, '1.427018e-01'), (181, '8.043408e-02'), (182, '9.711044e-02'), (183, '8.317285e-02'), (184, '6.425986e-02'), (185, '4.679310e-02'), (186, '3.362553e-01'), (187, '7.547653e-02'), (188, '6.098840e-02'), (189, '1.110280e-01'), (190, '1.603629e-01'), (191, '8.220889e-02'), (192, '3.916227e-03'), (193, '2.336461e-01'), (194, '1.031064e-01'), (195, '5.769355e-02'), (196, '9.537159e-02'), (197, '3.005859e-01'), (198, '5.052155e-03'), (199, '3.153595e-01'), (200, '3.951995e-02'), (201, '6.487295e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '2.923019e-05'), (1, '1.418598e-05'), (2, '7.279230e-05'), (3, '1.609624e-05'), (4, '1.749673e-06'), (5, '1.537435e-06'), (6, '8.636504e-06'), (7, '2.022663e-06'), (8, '1.753946e-06'), (9, '1.055563e-05'), (10, '2.031593e-06'), (11, '2.003306e-06'), (12, '1.079704e-05'), (13, '1.221193e-06'), (14, '1.674172e-06'), (15, '9.832203e-06'), (16, '1.624005e-06'), (17, '1.824328e-06'), (18, '1.041604e-05'), (19, '1.510199e-06'), (20, '1.981279e-06'), (21, '9.587735e-06'), (22, '1.183970e-05'), (23, '4.612358e-04'), (24, '1.630087e-04'), (25, '4.215305e-05'), (26, '7.178058e-06'), (27, '7.245569e-05'), (28, '1.119868e-05'), (29, '3.858187e-04'), (30, '1.005857e-04'), (31, '3.381688e-05'), (32, '3.973894e-06'), (33, '8.997152e-06'), (34, '8.258851e-06'), (35, '7.468972e-06'), (36, '7.629387e-06'), (37, '1.696329e-05'), (38, '2.906666e-06'), (39, '2.580753e-06'), (40, '1.486400e-05'), (41, '5.581116e-06'), (42, '6.853663e-06'), (43, '9.765329e-06'), (44, '5.539364e-06'), (45, '7.048814e-06'), (46, '1.186554e-05'), (47, '5.842205e-06'), (48, '1.275411e-05'), (49, '1.037618e-05'), (50, '1.362484e-05'), (51, '3.824230e-04'), (52, '8.607144e-05'), (53, '2.733000e-05'), (54, '3.025971e-06'), (55, '2.525433e-05'), (56, '5.151364e-06'), (57, '3.776101e-04'), (58, '1.642502e-04'), (59, '4.834505e-05'), (60, '7.496816e-06'), (61, '1.205686e-05'), (62, '1.372488e-05'), (63, '1.569287e-05'), (64, '1.521515e-05'), (65, '1.917815e-05'), (66, '8.514124e-06'), (67, '1.607305e-05'), (68, '1.144581e-05'), (69, '1.026167e-05'), (70, '1.313220e-05'), (71, '1.549338e-05'), (72, '1.248064e-05'), (73, '1.309549e-05'), (74, '1.812657e-05'), (75, '9.995261e-06'), (76, '1.000362e-05'), (77, '1.901101e-05'), (78, '1.768569e-05'), (79, '3.368045e-04'), (80, '6.369926e-05'), (81, '3.166577e-05'), (82, '3.043842e-06'), (83, '2.496912e-05'), (84, '4.001283e-06'), (85, '3.268775e-04'), (86, '1.435371e-04'), (87, '3.818672e-05'), (88, '2.453648e-05'), (89, '3.053775e-05'), (90, '1.808144e-05'), (91, '2.371118e-05'), (92, '3.515212e-05'), (93, '1.870925e-05'), (94, '7.011964e-06'), (95, '9.687836e-06'), (96, '1.579294e-05'), (97, '1.841080e-05'), (98, '2.319835e-05'), (99, '2.654162e-05'), (100, '3.272525e-06'), (101, '5.963192e-06'), (102, '7.134919e-06'), (103, '3.188194e-06'), (104, '6.292415e-06'), (105, '9.618213e-06'), (106, '1.459241e-05'), (107, '3.325663e-04'), (108, '5.623465e-05'), (109, '3.588581e-05'), (110, '3.646880e-06'), (111, '3.117020e-05'), (112, '3.871397e-06'), (113, '3.175976e-04'), (114, '2.007989e-04'), (115, '5.773743e-05'), (116, '9.718704e-06'), (117, '2.453076e-05'), (118, '2.252144e-05'), (119, '2.101204e-05'), (120, '4.464234e-05'), (121, '1.834713e-05'), (122, '2.343847e-05'), (123, '2.548385e-05'), (124, '2.425596e-05'), (125, '2.593292e-05'), (126, '4.364972e-05'), (127, '2.224572e-05'), (128, '2.324060e-05'), (129, '3.316263e-05'), (130, '2.155747e-05'), (131, '1.808847e-05'), (132, '1.550691e-05'), (133, '2.846708e-05'), (134, '2.126743e-05'), (135, '3.443270e-04'), (136, '6.085969e-05'), (137, '4.661737e-05'), (138, '5.154281e-06'), (139, '5.223293e-05'), (140, '4.159741e-06'), (141, '3.891710e-04'), (142, '2.745648e-04'), (143, '6.741406e-05'), (144, '4.145357e-05'), (145, '2.721878e-05'), (146, '4.516780e-05'), (147, '2.995153e-05'), (148, '1.965023e-05'), (149, '4.254690e-05'), (150, '4.431373e-05'), (151, '2.298501e-05'), (152, '5.016432e-05'), (153, '5.227009e-05'), (154, '2.097822e-05'), (155, '4.594243e-05'), (156, '2.853332e-05'), (157, '1.531971e-05'), (158, '5.228651e-05'), (159, '3.495847e-05'), (160, '2.910538e-05'), (161, '3.832636e-05'), (162, '2.415583e-05'), (163, '2.817802e-04'), (164, '1.010868e-04'), (165, '5.190976e-05'), (166, '1.243001e-05'), (167, '9.679571e-05'), (168, '8.627377e-06'), (169, '2.763181e-04'), (170, '3.037115e-04'), (171, '1.617295e-04'), (172, '4.171236e-05'), (173, '1.515857e-05'), (174, '5.666626e-05'), (175, '4.027194e-05'), (176, '1.943965e-05'), (177, '5.768607e-05'), (178, '5.126871e-05'), (179, '1.803040e-05'), (180, '6.279076e-05'), (181, '3.976913e-05'), (182, '1.824175e-05'), (183, '6.778354e-05'), (184, '3.821319e-05'), (185, '1.700145e-05'), (186, '7.556005e-05'), (187, '4.506252e-05'), (188, '1.959435e-05'), (189, '5.502275e-05'), (190, '3.881474e-05'), (191, '2.083055e-04'), (192, '1.000103e-04'), (193, '4.589289e-05'), (194, '4.774615e-05'), (195, '1.907016e-04'), (196, '2.006890e-05'), (197, '9.577606e-05'), (198, '3.275540e-04'), (199, '2.195469e-05'), (200, '1.152678e-04'), (201, '6.070643e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.364562e+00 121.537s| epoch: 10 | grad test: 2.939703e-05,2.939704e-05\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.345894e-02'), (1, '8.601277e-02'), (2, '1.873257e-02'), (3, '1.998301e-01'), (4, '1.826557e+00'), (5, '1.213133e-01'), (6, '1.162704e-01'), (7, '1.857057e-01'), (8, '1.649754e-01'), (9, '1.658234e-01'), (10, '1.303816e-01'), (11, '1.474993e-01'), (12, '1.487291e-01'), (13, '1.640175e-01'), (14, '2.051135e-01'), (15, '2.083483e-01'), (16, '1.380221e-01'), (17, '2.090507e-01'), (18, '1.329818e-01'), (19, '2.200111e-01'), (20, '1.782379e-01'), (21, '1.263366e-01'), (22, '1.848119e-01'), (23, '2.153793e-02'), (24, '1.006242e-02'), (25, '1.706329e-01'), (26, '1.296191e-01'), (27, '2.025824e-01'), (28, '1.440771e-01'), (29, '1.313351e-01'), (30, '1.121224e-02'), (31, '1.808829e-01'), (32, '2.371334e-01'), (33, '1.411488e-01'), (34, '2.362633e-01'), (35, '2.118059e-01'), (36, '1.615908e-01'), (37, '1.920464e-01'), (38, '9.052145e-02'), (39, '1.632245e-01'), (40, '1.794052e-01'), (41, '1.059776e-01'), (42, '2.576943e-01'), (43, '1.721642e-01'), (44, '1.911704e-01'), (45, '1.698360e-01'), (46, '1.387996e-01'), (47, '1.137516e-01'), (48, '1.016036e-01'), (49, '2.348329e-01'), (50, '3.190841e-01'), (51, '7.288215e-02'), (52, '9.390718e-03'), (53, '1.792340e-01'), (54, '4.556305e-02'), (55, '1.051928e-02'), (56, '5.827490e-02'), (57, '5.706851e-02'), (58, '1.170153e-02'), (59, '1.547362e-01'), (60, '1.630259e-01'), (61, '1.605530e-01'), (62, '3.361515e-01'), (63, '9.719517e-02'), (64, '1.146994e-01'), (65, '1.078712e-01'), (66, '1.477368e-01'), (67, '1.588995e-01'), (68, '1.444921e-01'), (69, '6.186048e-01'), (70, '1.094041e-01'), (71, '9.550777e-02'), (72, '1.248121e-01'), (73, '1.686819e-01'), (74, '1.397779e-01'), (75, '2.138986e-01'), (76, '2.399778e-01'), (77, '9.458425e-02'), (78, '1.433893e-01'), (79, '4.456491e-02'), (80, '8.844915e-03'), (81, '5.623399e-01'), (82, '7.249039e-02'), (83, '7.770610e-03'), (84, '4.765301e-02'), (85, '4.589833e-02'), (86, '1.154538e-02'), (87, '1.186872e-01'), (88, '2.331814e-01'), (89, '1.029055e-01'), (90, '1.768796e-01'), (91, '7.466342e-01'), (92, '1.558639e-01'), (93, '1.483699e+00'), (94, '1.347685e-01'), (95, '2.505255e-01'), (96, '9.488723e-02'), (97, '9.723400e-02'), (98, '1.563918e-01'), (99, '9.745055e-02'), (100, '3.660222e-01'), (101, '1.131318e-01'), (102, '2.244446e-01'), (103, '9.159286e-02'), (104, '1.675286e+00'), (105, '2.545459e-01'), (106, '1.625805e-01'), (107, '4.031906e-02'), (108, '1.041411e-02'), (109, '1.351615e-01'), (110, '1.043449e-01'), (111, '1.161216e-02'), (112, '1.189215e-01'), (113, '1.968377e-01'), (114, '9.113264e-03'), (115, '2.101087e-01'), (116, '1.369980e-01'), (117, '1.001217e-01'), (118, '1.261168e-01'), (119, '1.865644e-01'), (120, '1.428287e-01'), (121, '1.165796e-01'), (122, '8.923706e-02'), (123, '2.583978e-01'), (124, '1.634607e-01'), (125, '1.117238e-01'), (126, '1.246920e-01'), (127, '9.484382e-02'), (128, '8.762592e-02'), (129, '1.315646e-01'), (130, '9.051979e-02'), (131, '1.267929e-01'), (132, '9.143692e-02'), (133, '9.151579e-02'), (134, '7.118375e-01'), (135, '6.967815e-02'), (136, '9.685596e-03'), (137, '1.257263e-01'), (138, '9.491517e-02'), (139, '3.477266e-02'), (140, '7.157142e-02'), (141, '1.996776e-01'), (142, '7.154494e-03'), (143, '2.324593e-01'), (144, '7.712400e-02'), (145, '1.190997e+01'), (146, '8.278196e-01'), (147, '2.146697e-01'), (148, '8.970790e-02'), (149, '1.190435e-01'), (150, '9.685943e-02'), (151, '3.540541e-01'), (152, '9.458595e-02'), (153, '6.448821e-02'), (154, '5.460938e-02'), (155, '9.724028e-02'), (156, '8.098070e-02'), (157, '9.629966e-02'), (158, '1.019791e-01'), (159, '7.290765e-02'), (160, '8.961312e-02'), (161, '2.502840e-01'), (162, '1.296604e-01'), (163, '6.352732e-02'), (164, '7.796693e-03'), (165, '2.544009e-01'), (166, '8.472072e-02'), (167, '4.596985e-02'), (168, '6.779341e-02'), (169, '6.363500e-02'), (170, '5.908747e-03'), (171, '2.217518e-01'), (172, '7.851971e-02'), (173, '8.018120e-02'), (174, '1.383833e-01'), (175, '9.383593e-02'), (176, '1.860704e-01'), (177, '1.838874e-01'), (178, '8.575439e-02'), (179, '7.012695e-02'), (180, '9.748731e-02'), (181, '9.746110e-02'), (182, '9.669924e-02'), (183, '9.046113e-02'), (184, '7.214155e-02'), (185, '2.151706e-01'), (186, '1.424406e-01'), (187, '8.611177e-02'), (188, '6.223065e-02'), (189, '1.457122e-01'), (190, '2.582386e-01'), (191, '8.758637e-02'), (192, '3.950153e-03'), (193, '1.590094e-01'), (194, '1.246477e-01'), (195, '6.188891e-02'), (196, '7.753444e-02'), (197, '3.040479e-01'), (198, '5.094349e-03'), (199, '1.378079e-01'), (200, '3.152477e-02'), (201, '6.762878e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '3.782890e-05'), (1, '1.651654e-05'), (2, '8.090126e-05'), (3, '2.087250e-05'), (4, '2.139159e-06'), (5, '1.835423e-06'), (6, '1.493478e-05'), (7, '2.424733e-06'), (8, '2.070251e-06'), (9, '1.612782e-05'), (10, '2.024517e-06'), (11, '2.267965e-06'), (12, '1.499967e-05'), (13, '1.886717e-06'), (14, '2.044026e-06'), (15, '1.525764e-05'), (16, '2.498987e-06'), (17, '1.657141e-06'), (18, '1.551693e-05'), (19, '1.957095e-06'), (20, '1.743972e-06'), (21, '1.581636e-05'), (22, '1.677070e-05'), (23, '6.568833e-04'), (24, '1.615428e-04'), (25, '4.707796e-05'), (26, '8.464752e-06'), (27, '8.897892e-05'), (28, '1.341527e-05'), (29, '5.736995e-04'), (30, '9.534290e-05'), (31, '3.217148e-05'), (32, '1.139853e-05'), (33, '9.368845e-06'), (34, '1.482734e-05'), (35, '1.158416e-05'), (36, '1.063378e-05'), (37, '2.076866e-05'), (38, '4.475862e-06'), (39, '2.388847e-06'), (40, '1.683717e-05'), (41, '6.320478e-06'), (42, '7.024175e-06'), (43, '1.148640e-05'), (44, '7.101511e-06'), (45, '7.854653e-06'), (46, '1.687548e-05'), (47, '1.002231e-05'), (48, '1.083022e-05'), (49, '2.310998e-05'), (50, '1.826155e-05'), (51, '5.575000e-04'), (52, '8.046081e-05'), (53, '2.534115e-05'), (54, '3.061958e-06'), (55, '2.529595e-05'), (56, '5.281554e-06'), (57, '5.307637e-04'), (58, '1.491526e-04'), (59, '4.311032e-05'), (60, '9.267906e-06'), (61, '1.718900e-05'), (62, '1.764456e-05'), (63, '1.306810e-05'), (64, '1.569711e-05'), (65, '2.097525e-05'), (66, '1.582796e-05'), (67, '2.500840e-05'), (68, '1.858463e-05'), (69, '1.707261e-05'), (70, '1.983566e-05'), (71, '2.073647e-05'), (72, '1.327879e-05'), (73, '1.048610e-05'), (74, '2.258382e-05'), (75, '1.077928e-05'), (76, '8.544560e-06'), (77, '2.275379e-05'), (78, '1.998023e-05'), (79, '5.080301e-04'), (80, '5.678694e-05'), (81, '2.306336e-05'), (82, '3.013299e-06'), (83, '2.491287e-05'), (84, '3.819635e-06'), (85, '5.032722e-04'), (86, '1.653245e-04'), (87, '3.742701e-05'), (88, '2.949730e-05'), (89, '2.619409e-05'), (90, '3.513285e-05'), (91, '2.834078e-05'), (92, '3.469806e-05'), (93, '2.644438e-05'), (94, '1.109621e-05'), (95, '1.080261e-05'), (96, '3.060429e-05'), (97, '2.582953e-05'), (98, '3.247835e-05'), (99, '2.999681e-05'), (100, '5.684291e-06'), (101, '7.270552e-06'), (102, '5.905830e-06'), (103, '3.523330e-06'), (104, '7.441341e-06'), (105, '8.460714e-06'), (106, '2.056332e-05'), (107, '4.583546e-04'), (108, '5.327288e-05'), (109, '3.673257e-05'), (110, '4.070513e-06'), (111, '3.530185e-05'), (112, '3.144618e-06'), (113, '4.623566e-04'), (114, '2.085063e-04'), (115, '5.909681e-05'), (116, '1.175958e-05'), (117, '2.483039e-05'), (118, '2.213981e-05'), (119, '2.259970e-05'), (120, '4.822292e-05'), (121, '1.436026e-05'), (122, '2.477323e-05'), (123, '2.990286e-05'), (124, '2.837823e-05'), (125, '2.170030e-05'), (126, '3.636589e-05'), (127, '2.039349e-05'), (128, '3.364464e-05'), (129, '4.587812e-05'), (130, '2.697361e-05'), (131, '2.013321e-05'), (132, '1.712110e-05'), (133, '2.815109e-05'), (134, '2.116716e-05'), (135, '4.708971e-04'), (136, '5.232092e-05'), (137, '3.265532e-05'), (138, '4.653618e-06'), (139, '4.256728e-05'), (140, '3.527475e-06'), (141, '4.825636e-04'), (142, '2.803704e-04'), (143, '7.123329e-05'), (144, '3.280418e-05'), (145, '2.012046e-05'), (146, '5.732830e-05'), (147, '4.383893e-05'), (148, '2.598204e-05'), (149, '5.361152e-05'), (150, '4.556125e-05'), (151, '3.786988e-05'), (152, '6.311505e-05'), (153, '4.038733e-05'), (154, '2.803072e-05'), (155, '4.872396e-05'), (156, '2.975905e-05'), (157, '2.174160e-05'), (158, '5.874990e-05'), (159, '3.347225e-05'), (160, '3.997910e-05'), (161, '3.453305e-05'), (162, '2.758967e-05'), (163, '3.669779e-04'), (164, '9.314406e-05'), (165, '4.759990e-05'), (166, '1.283339e-05'), (167, '1.011157e-04'), (168, '8.863155e-06'), (169, '4.089507e-04'), (170, '3.011383e-04'), (171, '1.486114e-04'), (172, '4.913967e-05'), (173, '1.857656e-05'), (174, '6.317279e-05'), (175, '4.272264e-05'), (176, '2.260363e-05'), (177, '8.392667e-05'), (178, '6.722651e-05'), (179, '1.803757e-05'), (180, '8.366153e-05'), (181, '3.849202e-05'), (182, '2.033071e-05'), (183, '7.618465e-05'), (184, '4.722358e-05'), (185, '2.092713e-05'), (186, '9.166694e-05'), (187, '5.797382e-05'), (188, '1.953532e-05'), (189, '8.009899e-05'), (190, '4.019611e-05'), (191, '2.781037e-04'), (192, '9.667928e-05'), (193, '4.303459e-05'), (194, '4.828623e-05'), (195, '2.068913e-04'), (196, '1.984974e-05'), (197, '1.202703e-04'), (198, '3.179850e-04'), (199, '1.625173e-05'), (200, '1.204508e-04'), (201, '7.218863e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.327757e+00 214.112s| epoch: 20 | grad test: 5.781092e-04,5.781090e-04\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.465979e-02'), (1, '7.587332e-02'), (2, '2.060226e-02'), (3, '1.709926e-01'), (4, '2.034017e-01'), (5, '3.149596e-01'), (6, '1.723097e-01'), (7, '1.176011e-01'), (8, '1.332763e-01'), (9, '1.459552e-01'), (10, '1.171323e-01'), (11, '1.527770e-01'), (12, '3.171244e-01'), (13, '1.822616e+00'), (14, '2.627321e-01'), (15, '2.088333e-01'), (16, '1.350532e-01'), (17, '1.555442e-01'), (18, '1.946627e-01'), (19, '1.258363e-01'), (20, '1.047387e+00'), (21, '1.293902e-01'), (22, '2.206801e-01'), (23, '2.666705e-02'), (24, '9.793662e-03'), (25, '1.896244e-01'), (26, '1.151804e-01'), (27, '4.624990e-02'), (28, '1.671312e-01'), (29, '6.860287e-02'), (30, '1.175816e-02'), (31, '1.205470e-01'), (32, '2.275758e-01'), (33, '1.442810e-01'), (34, '1.635380e-01'), (35, '1.093078e-01'), (36, '1.343334e-01'), (37, '1.318189e-01'), (38, '1.060979e-01'), (39, '8.799215e-02'), (40, '1.394224e-01'), (41, '2.944333e-01'), (42, '9.316774e-02'), (43, '2.233905e-01'), (44, '1.987901e-01'), (45, '1.201380e-01'), (46, '1.253020e-01'), (47, '1.300430e-01'), (48, '1.816174e-01'), (49, '1.762090e-01'), (50, '1.583574e-01'), (51, '6.366606e-02'), (52, '9.491079e-03'), (53, '1.541802e-01'), (54, '4.630687e-02'), (55, '1.151564e-02'), (56, '5.289100e-02'), (57, '6.570269e-02'), (58, '1.165938e-02'), (59, '2.070352e-01'), (60, '3.435909e-01'), (61, '1.250462e-01'), (62, '1.975722e-01'), (63, '1.043344e-01'), (64, '1.267625e-01'), (65, '1.585098e-01'), (66, '9.267759e-02'), (67, '4.496429e-01'), (68, '1.221424e-01'), (69, '2.379709e-01'), (70, '1.184598e-01'), (71, '1.793009e-01'), (72, '1.292309e-01'), (73, '9.355151e-02'), (74, '1.025643e-01'), (75, '2.279639e-01'), (76, '1.106425e-01'), (77, '1.317247e-01'), (78, '1.353163e-01'), (79, '6.323864e-02'), (80, '9.360506e-03'), (81, '1.568759e-01'), (82, '5.216631e-02'), (83, '1.020817e-02'), (84, '4.396237e-02'), (85, '5.485418e-02'), (86, '1.151719e-02'), (87, '1.864314e-01'), (88, '1.175885e-01'), (89, '1.221625e-01'), (90, '1.682065e-01'), (91, '1.803612e-01'), (92, '1.309500e-01'), (93, '1.875241e-01'), (94, '1.809070e-01'), (95, '1.697724e-01'), (96, '2.126249e-01'), (97, '1.530373e-01'), (98, '1.334636e-01'), (99, '3.052059e-01'), (100, '8.429269e-02'), (101, '1.149948e-01'), (102, '1.668766e-01'), (103, '1.323007e-01'), (104, '2.228017e-01'), (105, '2.407970e-01'), (106, '1.521924e-01'), (107, '5.974979e-02'), (108, '9.534421e-03'), (109, '2.261839e-01'), (110, '9.397429e-02'), (111, '1.263513e-02'), (112, '7.652531e-02'), (113, '2.321917e-01'), (114, '8.534381e-03'), (115, '1.536432e-01'), (116, '1.211343e-01'), (117, '1.157227e-01'), (118, '1.739857e-01'), (119, '1.203692e-01'), (120, '8.788833e-01'), (121, '1.636698e-01'), (122, '1.168319e-01'), (123, '1.338146e-01'), (124, '1.156802e-01'), (125, '1.085012e-01'), (126, '8.420976e-02'), (127, '1.225766e-01'), (128, '9.902976e-02'), (129, '5.028918e+00'), (130, '1.347959e-01'), (131, '8.950178e-02'), (132, '1.580302e-01'), (133, '1.578791e-01'), (134, '1.832038e-01'), (135, '1.575997e-01'), (136, '1.087322e-02'), (137, '1.298562e-01'), (138, '2.104363e-01'), (139, '1.948744e-02'), (140, '1.576823e-01'), (141, '7.984377e-02'), (142, '7.029404e-03'), (143, '1.542495e-01'), (144, '1.141921e-01'), (145, '2.168678e-01'), (146, '9.362010e-02'), (147, '1.020854e-01'), (148, '1.154277e-01'), (149, '2.705365e-01'), (150, '1.538874e-01'), (151, '9.027392e-02'), (152, '9.092661e-02'), (153, '6.698190e-02'), (154, '1.144305e-01'), (155, '8.235572e-02'), (156, '9.736233e-02'), (157, '7.539880e-02'), (158, '6.937883e-02'), (159, '7.775557e-02'), (160, '1.459228e-01'), (161, '9.744506e-02'), (162, '1.409730e+00'), (163, '9.341661e-02'), (164, '7.818459e-03'), (165, '1.547738e-01'), (166, '3.159055e-01'), (167, '1.336295e-01'), (168, '6.860037e-02'), (169, '6.255232e-02'), (170, '5.861245e-03'), (171, '7.711146e-02'), (172, '7.823180e-02'), (173, '9.861780e-02'), (174, '1.093284e-01'), (175, '2.478817e-01'), (176, '6.967386e-02'), (177, '1.640657e-01'), (178, '7.539052e-02'), (179, '5.877415e-02'), (180, '2.939553e-01'), (181, '7.126223e-02'), (182, '8.277670e-02'), (183, '1.108659e-01'), (184, '1.516376e+00'), (185, '4.479676e-02'), (186, '1.053077e-01'), (187, '1.986564e-01'), (188, '6.849279e-02'), (189, '1.026529e-01'), (190, '1.603527e-01'), (191, '5.850317e-02'), (192, '3.960538e-03'), (193, '2.005063e-01'), (194, '1.529184e-01'), (195, '3.497249e-01'), (196, '9.351920e-02'), (197, '2.271245e-01'), (198, '5.122499e-03'), (199, '1.702523e-01'), (200, '4.442741e-02'), (201, '8.313032e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '3.545543e-05'), (1, '1.680929e-05'), (2, '8.606660e-05'), (3, '1.805902e-05'), (4, '1.796635e-06'), (5, '2.006008e-06'), (6, '1.361847e-05'), (7, '1.893542e-06'), (8, '1.933664e-06'), (9, '1.537924e-05'), (10, '2.349760e-06'), (11, '2.384747e-06'), (12, '1.489295e-05'), (13, '2.068443e-06'), (14, '1.898261e-06'), (15, '1.543078e-05'), (16, '2.004262e-06'), (17, '2.135166e-06'), (18, '1.395546e-05'), (19, '1.344491e-06'), (20, '1.611785e-06'), (21, '1.498319e-05'), (22, '1.842718e-05'), (23, '6.115916e-04'), (24, '1.603951e-04'), (25, '4.500836e-05'), (26, '7.918262e-06'), (27, '8.240958e-05'), (28, '1.233331e-05'), (29, '5.288638e-04'), (30, '1.138356e-04'), (31, '3.879038e-05'), (32, '8.233324e-06'), (33, '8.430591e-06'), (34, '1.872116e-05'), (35, '8.343719e-06'), (36, '8.520430e-06'), (37, '1.840290e-05'), (38, '4.501393e-06'), (39, '3.487537e-06'), (40, '1.568116e-05'), (41, '6.897091e-06'), (42, '6.495388e-06'), (43, '1.723665e-05'), (44, '9.369785e-06'), (45, '9.096049e-06'), (46, '1.737438e-05'), (47, '1.029468e-05'), (48, '1.018812e-05'), (49, '2.772151e-05'), (50, '1.933084e-05'), (51, '4.590178e-04'), (52, '8.460887e-05'), (53, '3.547928e-05'), (54, '3.429378e-06'), (55, '2.926782e-05'), (56, '5.165191e-06'), (57, '4.571090e-04'), (58, '1.447616e-04'), (59, '4.303621e-05'), (60, '1.258977e-05'), (61, '2.037476e-05'), (62, '2.550436e-05'), (63, '1.485501e-05'), (64, '1.496175e-05'), (65, '2.514501e-05'), (66, '1.143668e-05'), (67, '1.827120e-05'), (68, '2.210284e-05'), (69, '1.335454e-05'), (70, '1.431214e-05'), (71, '2.286808e-05'), (72, '1.141830e-05'), (73, '1.075329e-05'), (74, '2.112346e-05'), (75, '9.224418e-06'), (76, '8.525661e-06'), (77, '2.114919e-05'), (78, '1.759240e-05'), (79, '3.991004e-04'), (80, '6.234431e-05'), (81, '2.735705e-05'), (82, '3.148576e-06'), (83, '2.599574e-05'), (84, '3.788435e-06'), (85, '3.927138e-04'), (86, '1.629417e-04'), (87, '5.640208e-05'), (88, '2.117317e-05'), (89, '2.515426e-05'), (90, '3.235222e-05'), (91, '2.474206e-05'), (92, '3.277632e-05'), (93, '2.662568e-05'), (94, '1.204823e-05'), (95, '1.331185e-05'), (96, '2.678375e-05'), (97, '2.214998e-05'), (98, '1.956360e-05'), (99, '3.303015e-05'), (100, '3.299028e-06'), (101, '6.612839e-06'), (102, '5.597295e-06'), (103, '5.281567e-06'), (104, '1.451476e-05'), (105, '8.353991e-06'), (106, '1.734710e-05'), (107, '3.600169e-04'), (108, '6.135730e-05'), (109, '4.697625e-05'), (110, '3.873919e-06'), (111, '3.340002e-05'), (112, '3.471519e-06'), (113, '3.264899e-04'), (114, '2.228743e-04'), (115, '6.883908e-05'), (116, '2.554482e-05'), (117, '3.415279e-05'), (118, '2.889956e-05'), (119, '1.695935e-05'), (120, '3.616952e-05'), (121, '1.735406e-05'), (122, '2.540885e-05'), (123, '2.594415e-05'), (124, '2.995227e-05'), (125, '2.383539e-05'), (126, '3.888451e-05'), (127, '2.163380e-05'), (128, '2.913630e-05'), (129, '4.010811e-05'), (130, '2.970094e-05'), (131, '1.960282e-05'), (132, '2.317325e-05'), (133, '2.952254e-05'), (134, '2.121373e-05'), (135, '3.412875e-04'), (136, '6.567864e-05'), (137, '5.640117e-05'), (138, '8.133664e-06'), (139, '5.905337e-05'), (140, '5.217561e-06'), (141, '3.259627e-04'), (142, '3.004989e-04'), (143, '7.837417e-05'), (144, '4.499530e-05'), (145, '2.344374e-05'), (146, '5.555631e-05'), (147, '3.261294e-05'), (148, '1.993422e-05'), (149, '4.362589e-05'), (150, '3.178563e-05'), (151, '2.200749e-05'), (152, '4.737394e-05'), (153, '3.783706e-05'), (154, '2.848800e-05'), (155, '4.732160e-05'), (156, '2.573704e-05'), (157, '1.727117e-05'), (158, '4.118590e-05'), (159, '2.118447e-05'), (160, '3.035976e-05'), (161, '3.241012e-05'), (162, '2.680323e-05'), (163, '2.729516e-04'), (164, '1.015304e-04'), (165, '4.569037e-05'), (166, '1.383428e-05'), (167, '9.851922e-05'), (168, '8.878130e-06'), (169, '2.653182e-04'), (170, '3.161093e-04'), (171, '1.636466e-04'), (172, '4.365272e-05'), (173, '1.729784e-05'), (174, '6.391075e-05'), (175, '4.658735e-05'), (176, '2.315133e-05'), (177, '6.406119e-05'), (178, '5.755068e-05'), (179, '2.010213e-05'), (180, '7.196544e-05'), (181, '3.717092e-05'), (182, '1.818564e-05'), (183, '6.315897e-05'), (184, '5.150177e-05'), (185, '2.099861e-05'), (186, '8.283916e-05'), (187, '4.993056e-05'), (188, '2.026680e-05'), (189, '6.617101e-05'), (190, '4.117716e-05'), (191, '2.148194e-04'), (192, '1.009258e-04'), (193, '4.615108e-05'), (194, '4.937921e-05'), (195, '2.072033e-04'), (196, '2.053521e-05'), (197, '1.305078e-04'), (198, '3.206076e-04'), (199, '2.597395e-05'), (200, '1.311213e-04'), (201, '8.132299e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.357055e+00 295.248s| epoch: 30 | grad test: 2.283194e-04,2.283194e-04\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.623163e-02'), (1, '1.135157e-01'), (2, '3.343857e-02'), (3, '1.324810e+00'), (4, '2.582977e-01'), (5, '1.591523e-01'), (6, '1.382741e-01'), (7, '1.305694e-01'), (8, '9.794379e-02'), (9, '1.304210e-01'), (10, '1.050462e-01'), (11, '1.185605e-01'), (12, '1.863629e-01'), (13, '1.582666e-01'), (14, '1.624226e-01'), (15, '1.823607e-01'), (16, '1.305090e-01'), (17, '1.248044e-01'), (18, '1.697028e-01'), (19, '2.425422e-01'), (20, '1.763661e-01'), (21, '1.558811e-01'), (22, '5.844023e-01'), (23, '2.526568e-02'), (24, '1.017748e-02'), (25, '1.210718e-01'), (26, '1.159386e-01'), (27, '6.815993e-02'), (28, '1.042109e-01'), (29, '5.129367e-02'), (30, '1.214869e-02'), (31, '1.248465e-01'), (32, '2.411859e-01'), (33, '2.546021e-01'), (34, '1.525869e-01'), (35, '1.012118e-01'), (36, '1.444273e-01'), (37, '1.508590e-01'), (38, '1.272275e-01'), (39, '1.138593e-01'), (40, '1.465994e-01'), (41, '1.597750e-01'), (42, '1.748611e-01'), (43, '3.694942e-01'), (44, '1.412081e-01'), (45, '1.153572e-01'), (46, '1.161190e-01'), (47, '7.964876e-02'), (48, '2.022727e-01'), (49, '1.630630e-01'), (50, '1.734374e-01'), (51, '4.790462e-02'), (52, '9.493778e-03'), (53, '1.796474e+00'), (54, '1.351621e-01'), (55, '1.525386e-02'), (56, '2.611193e-01'), (57, '3.202581e-02'), (58, '1.289641e-02'), (59, '1.908656e-01'), (60, '1.121421e-01'), (61, '1.012065e-01'), (62, '1.335936e-01'), (63, '9.054279e-02'), (64, '1.033316e-01'), (65, '1.220219e-01'), (66, '1.019901e+00'), (67, '1.317677e-01'), (68, '1.921487e-01'), (69, '1.248778e-01'), (70, '1.381604e-01'), (71, '1.145025e-01'), (72, '1.342416e-01'), (73, '1.519559e-01'), (74, '1.231218e-01'), (75, '6.315597e-01'), (76, '1.380734e-01'), (77, '1.255593e-01'), (78, '1.730127e-01'), (79, '4.138238e-02'), (80, '4.392460e-02'), (81, '1.661505e-01'), (82, '9.146413e-02'), (83, '8.854727e-03'), (84, '4.742200e-02'), (85, '4.272043e-02'), (86, '1.201173e-02'), (87, '2.056129e-01'), (88, '8.989716e-02'), (89, '1.218842e-01'), (90, '8.933335e-02'), (91, '9.295951e-01'), (92, '9.378840e-02'), (93, '1.802246e-01'), (94, '1.418922e-01'), (95, '1.337074e-01'), (96, '9.914083e-02'), (97, '1.204189e-01'), (98, '1.868801e-01'), (99, '9.398106e-02'), (100, '9.022469e-02'), (101, '1.074077e-01'), (102, '1.569274e-01'), (103, '1.204377e-01'), (104, '1.434708e-01'), (105, '1.356869e-01'), (106, '1.829500e-01'), (107, '5.871098e-02'), (108, '1.039875e-02'), (109, '1.514493e-01'), (110, '8.201723e-02'), (111, '1.674023e-02'), (112, '8.782964e-02'), (113, '5.220305e-02'), (114, '9.097600e-03'), (115, '1.970518e-01'), (116, '7.455110e-02'), (117, '1.023745e-01'), (118, '1.026778e-01'), (119, '1.351462e-01'), (120, '1.673919e-01'), (121, '1.255257e-01'), (122, '1.276250e-01'), (123, '8.681401e-02'), (124, '1.151938e-01'), (125, '1.358774e-01'), (126, '8.864679e-02'), (127, '2.172152e-01'), (128, '1.304836e-01'), (129, '8.696163e-02'), (130, '1.377453e-01'), (131, '1.212240e-01'), (132, '1.591111e-01'), (133, '1.266888e-01'), (134, '1.560378e-01'), (135, '1.698391e-01'), (136, '9.975736e-03'), (137, '1.596125e-01'), (138, '1.131935e-01'), (139, '1.977268e-02'), (140, '1.394305e-01'), (141, '1.097005e-01'), (142, '6.899036e-03'), (143, '3.572379e-01'), (144, '8.185083e-02'), (145, '6.296650e-02'), (146, '1.171357e-01'), (147, '7.869027e-02'), (148, '8.867586e-02'), (149, '8.611072e-02'), (150, '9.997644e-02'), (151, '6.255975e-02'), (152, '1.871655e-01'), (153, '7.051216e-02'), (154, '6.961752e-02'), (155, '1.051774e-01'), (156, '9.527407e-02'), (157, '1.138318e-01'), (158, '1.694954e-01'), (159, '1.632576e-01'), (160, '1.236584e-01'), (161, '1.233285e-01'), (162, '1.539610e-01'), (163, '8.706127e-02'), (164, '7.721589e-03'), (165, '1.573437e-01'), (166, '6.377809e-02'), (167, '5.581892e-02'), (168, '1.503287e-01'), (169, '6.418990e-02'), (170, '5.723534e-03'), (171, '5.328019e-02'), (172, '2.089479e-01'), (173, '1.012678e-01'), (174, '1.073559e-01'), (175, '8.549275e-02'), (176, '1.524479e-01'), (177, '6.918208e-02'), (178, '7.729907e-02'), (179, '1.766205e-01'), (180, '7.557863e-02'), (181, '6.000293e-02'), (182, '1.857549e-01'), (183, '1.003108e-01'), (184, '2.097549e+00'), (185, '5.338882e-02'), (186, '1.030854e-01'), (187, '8.543976e-02'), (188, '4.832553e-02'), (189, '9.020825e-02'), (190, '4.755738e-01'), (191, '5.033438e-02'), (192, '3.972418e-03'), (193, '2.636375e-01'), (194, '1.106478e-01'), (195, '3.645821e-02'), (196, '8.471492e-02'), (197, '7.029882e-02'), (198, '5.076317e-03'), (199, '1.801369e-01'), (200, '5.850577e-01'), (201, '4.841773e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '3.070023e-05'), (1, '1.467067e-05'), (2, '7.582027e-05'), (3, '2.540983e-05'), (4, '1.676342e-06'), (5, '1.970723e-06'), (6, '1.157041e-05'), (7, '1.921581e-06'), (8, '1.641899e-06'), (9, '1.253385e-05'), (10, '1.833234e-06'), (11, '1.858633e-06'), (12, '1.206401e-05'), (13, '1.401546e-06'), (14, '1.615807e-06'), (15, '1.091099e-05'), (16, '2.626902e-06'), (17, '1.662308e-06'), (18, '1.229149e-05'), (19, '1.397261e-06'), (20, '1.675658e-06'), (21, '1.228166e-05'), (22, '1.419153e-05'), (23, '5.395107e-04'), (24, '1.642610e-04'), (25, '3.866229e-05'), (26, '7.334724e-06'), (27, '7.962425e-05'), (28, '1.129280e-05'), (29, '4.704064e-04'), (30, '1.004000e-04'), (31, '3.060618e-05'), (32, '9.843257e-06'), (33, '1.139169e-05'), (34, '1.140675e-05'), (35, '6.050740e-06'), (36, '6.230654e-06'), (37, '1.717993e-05'), (38, '4.854323e-06'), (39, '3.203215e-06'), (40, '1.541459e-05'), (41, '4.699036e-06'), (42, '5.563034e-06'), (43, '1.391540e-05'), (44, '5.523387e-06'), (45, '5.159774e-06'), (46, '1.621635e-05'), (47, '5.921174e-06'), (48, '8.927137e-06'), (49, '1.580598e-05'), (50, '1.745416e-05'), (51, '4.401214e-04'), (52, '8.190585e-05'), (53, '3.361317e-05'), (54, '3.185932e-06'), (55, '3.124079e-05'), (56, '4.967632e-06'), (57, '4.204602e-04'), (58, '1.532328e-04'), (59, '5.707896e-05'), (60, '8.377112e-06'), (61, '1.455104e-05'), (62, '1.377068e-05'), (63, '1.227961e-05'), (64, '1.286095e-05'), (65, '2.094167e-05'), (66, '1.231793e-05'), (67, '2.414283e-05'), (68, '1.408367e-05'), (69, '1.359944e-05'), (70, '1.437962e-05'), (71, '1.732262e-05'), (72, '1.363285e-05'), (73, '1.225950e-05'), (74, '2.069378e-05'), (75, '1.116751e-05'), (76, '8.836155e-06'), (77, '2.154430e-05'), (78, '1.840636e-05'), (79, '3.861044e-04'), (80, '6.573287e-05'), (81, '3.453164e-05'), (82, '3.853408e-06'), (83, '2.753398e-05'), (84, '3.909643e-06'), (85, '3.695339e-04'), (86, '1.697909e-04'), (87, '5.497535e-05'), (88, '2.125162e-05'), (89, '2.998310e-05'), (90, '2.132907e-05'), (91, '2.228602e-05'), (92, '2.831437e-05'), (93, '1.506308e-05'), (94, '5.751359e-06'), (95, '8.152207e-06'), (96, '1.863324e-05'), (97, '3.572910e-05'), (98, '3.362818e-05'), (99, '2.753993e-05'), (100, '8.221779e-06'), (101, '9.846550e-06'), (102, '6.148056e-06'), (103, '6.012653e-06'), (104, '1.156728e-05'), (105, '7.807010e-06'), (106, '1.719745e-05'), (107, '3.651824e-04'), (108, '5.511464e-05'), (109, '3.182621e-05'), (110, '3.498726e-06'), (111, '3.143166e-05'), (112, '2.892762e-06'), (113, '3.658602e-04'), (114, '1.988965e-04'), (115, '7.743367e-05'), (116, '1.132006e-05'), (117, '2.549660e-05'), (118, '2.522006e-05'), (119, '1.995407e-05'), (120, '6.903490e-05'), (121, '1.480707e-05'), (122, '2.357366e-05'), (123, '3.015607e-05'), (124, '2.271881e-05'), (125, '1.999551e-05'), (126, '4.695297e-05'), (127, '1.884251e-05'), (128, '3.027754e-05'), (129, '4.336593e-05'), (130, '2.348715e-05'), (131, '2.194300e-05'), (132, '1.717401e-05'), (133, '2.900247e-05'), (134, '2.032645e-05'), (135, '3.693494e-04'), (136, '5.742597e-05'), (137, '3.892110e-05'), (138, '4.889314e-06'), (139, '4.139033e-05'), (140, '3.379700e-06'), (141, '3.705178e-04'), (142, '2.688553e-04'), (143, '6.201627e-05'), (144, '4.393732e-05'), (145, '1.883330e-05'), (146, '6.003896e-05'), (147, '2.965855e-05'), (148, '2.150848e-05'), (149, '4.690771e-05'), (150, '4.068917e-05'), (151, '2.191200e-05'), (152, '5.744022e-05'), (153, '3.570602e-05'), (154, '2.150001e-05'), (155, '4.993584e-05'), (156, '3.540024e-05'), (157, '1.674743e-05'), (158, '7.565354e-05'), (159, '3.234961e-05'), (160, '3.324273e-05'), (161, '4.749068e-05'), (162, '2.644060e-05'), (163, '3.516069e-04'), (164, '9.586443e-05'), (165, '5.808272e-05'), (166, '1.267712e-05'), (167, '9.680267e-05'), (168, '8.699634e-06'), (169, '3.813867e-04'), (170, '2.870080e-04'), (171, '1.593511e-04'), (172, '5.319882e-05'), (173, '1.571011e-05'), (174, '7.741631e-05'), (175, '3.787559e-05'), (176, '1.681537e-05'), (177, '7.270167e-05'), (178, '7.225736e-05'), (179, '1.854368e-05'), (180, '7.195761e-05'), (181, '2.989986e-05'), (182, '1.735909e-05'), (183, '7.628063e-05'), (184, '4.390877e-05'), (185, '1.965462e-05'), (186, '8.636594e-05'), (187, '7.105282e-05'), (188, '1.841402e-05'), (189, '7.673068e-05'), (190, '4.172824e-05'), (191, '2.787160e-04'), (192, '9.781313e-05'), (193, '4.967125e-05'), (194, '5.081594e-05'), (195, '2.198343e-04'), (196, '2.079624e-05'), (197, '1.556913e-04'), (198, '3.192750e-04'), (199, '2.641456e-05'), (200, '1.168726e-04'), (201, '7.554123e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.347030e+00 377.753s| epoch: 40 | grad test: -5.410648e-05,-5.410649e-05\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '3.062941e-02'), (1, '8.467957e-02'), (2, '1.931219e-02'), (3, '1.423085e-01'), (4, '1.816687e-01'), (5, '2.488082e-01'), (6, '3.569770e-01'), (7, '2.912739e-01'), (8, '1.279908e-01'), (9, '1.715993e-01'), (10, '1.622448e-01'), (11, '1.098685e-01'), (12, '1.344689e-01'), (13, '9.468462e-02'), (14, '7.675686e-01'), (15, '3.421133e-01'), (16, '1.140719e-01'), (17, '4.415819e-01'), (18, '1.883997e-01'), (19, '1.410974e-01'), (20, '1.852282e-01'), (21, '1.406258e-01'), (22, '1.693235e-01'), (23, '4.092632e-02'), (24, '1.017380e-02'), (25, '1.581621e-01'), (26, '1.513317e-01'), (27, '4.453495e-02'), (28, '1.391416e-01'), (29, '3.989487e-02'), (30, '1.367020e-02'), (31, '1.582152e-01'), (32, '1.750066e-01'), (33, '1.419437e-01'), (34, '1.649057e-01'), (35, '9.603545e-02'), (36, '2.180656e-01'), (37, '9.164486e-02'), (38, '9.180533e-02'), (39, '1.493714e-01'), (40, '1.242567e-01'), (41, '1.675436e-01'), (42, '1.478149e-01'), (43, '1.131658e-01'), (44, '1.257606e-01'), (45, '1.993469e-01'), (46, '1.366742e-01'), (47, '1.179502e-01'), (48, '2.986173e-01'), (49, '1.000814e+00'), (50, '1.798156e-01'), (51, '3.169430e-02'), (52, '1.031716e-02'), (53, '1.545390e-01'), (54, '1.400567e-01'), (55, '1.478790e-02'), (56, '8.521654e-02'), (57, '3.426779e-02'), (58, '1.131753e-02'), (59, '1.256780e-01'), (60, '1.040550e-01'), (61, '1.075389e-01'), (62, '1.336824e-01'), (63, '8.808988e-02'), (64, '1.399435e-01'), (65, '1.291187e-01'), (66, '1.040523e-01'), (67, '1.324535e-01'), (68, '1.330381e-01'), (69, '2.092457e-01'), (70, '1.675771e-01'), (71, '1.212897e-01'), (72, '1.300928e-01'), (73, '9.215536e-02'), (74, '1.974647e-01'), (75, '9.494270e-02'), (76, '2.089490e-01'), (77, '2.041712e-01'), (78, '1.236832e-01'), (79, '6.193890e-02'), (80, '9.697589e-03'), (81, '1.714046e-01'), (82, '7.253869e-02'), (83, '1.212649e-02'), (84, '9.258664e-02'), (85, '4.355483e-02'), (86, '1.354511e-02'), (87, '1.432041e-01'), (88, '2.000652e-01'), (89, '1.362289e-01'), (90, '1.333342e-01'), (91, '1.586067e-01'), (92, '1.295278e-01'), (93, '2.978165e-01'), (94, '1.267271e-01'), (95, '2.211082e-01'), (96, '1.361264e-01'), (97, '2.357305e-01'), (98, '1.229022e-01'), (99, '1.108774e-01'), (100, '2.158447e-01'), (101, '6.642184e-02'), (102, '1.313340e-01'), (103, '1.433653e-01'), (104, '1.512963e-01'), (105, '2.142297e-01'), (106, '2.477589e-01'), (107, '6.681921e-02'), (108, '1.148609e-02'), (109, '6.439595e-01'), (110, '8.642636e-02'), (111, '1.155130e-02'), (112, '7.337572e-02'), (113, '1.280275e-01'), (114, '9.708682e-03'), (115, '2.064617e-01'), (116, '8.048038e-02'), (117, '9.581351e-02'), (118, '1.573983e-01'), (119, '8.544170e-01'), (120, '7.148395e-02'), (121, '1.842676e-01'), (122, '1.020366e-01'), (123, '9.032489e-02'), (124, '1.402205e-01'), (125, '8.945476e-02'), (126, '1.176190e-01'), (127, '1.091026e-01'), (128, '9.442540e-02'), (129, '1.147668e-01'), (130, '1.078376e-01'), (131, '1.640483e-01'), (132, '1.046150e-01'), (133, '1.061882e-01'), (134, '1.254546e-01'), (135, '5.766936e-02'), (136, '9.599398e-03'), (137, '1.322199e-01'), (138, '7.174629e-02'), (139, '1.431180e-02'), (140, '2.037786e-01'), (141, '2.048941e-01'), (142, '7.064836e-03'), (143, '1.855174e-01'), (144, '9.264319e-02'), (145, '7.256161e-02'), (146, '1.361535e-01'), (147, '8.995710e-02'), (148, '7.973384e-02'), (149, '9.146059e-02'), (150, '1.434694e-01'), (151, '3.480743e-01'), (152, '7.417965e-02'), (153, '7.842005e-02'), (154, '4.673353e-02'), (155, '1.159257e-01'), (156, '2.616745e-01'), (157, '8.221156e-02'), (158, '1.019095e-01'), (159, '9.578169e-02'), (160, '1.022645e-01'), (161, '1.371847e-01'), (162, '1.304889e-01'), (163, '6.335023e-02'), (164, '7.794705e-03'), (165, '1.396427e-01'), (166, '6.155642e-02'), (167, '5.773224e-02'), (168, '6.071267e-02'), (169, '8.135632e-02'), (170, '5.807769e-03'), (171, '5.089562e-02'), (172, '6.154930e-02'), (173, '8.685075e-02'), (174, '1.033323e-01'), (175, '6.774634e-02'), (176, '8.494752e-02'), (177, '9.021065e-02'), (178, '8.077085e-02'), (179, '1.283401e-01'), (180, '1.558001e-01'), (181, '7.069543e-02'), (182, '7.343528e-02'), (183, '8.715337e-02'), (184, '7.214403e-02'), (185, '6.251934e-02'), (186, '1.864908e-01'), (187, '9.843856e-02'), (188, '5.639422e-02'), (189, '1.055191e-01'), (190, '1.181859e-01'), (191, '5.968973e-02'), (192, '3.942311e-03'), (193, '1.459403e-01'), (194, '7.986047e-02'), (195, '5.964949e-02'), (196, '8.257184e-02'), (197, '1.216693e-01'), (198, '5.116857e-03'), (199, '1.274930e-01'), (200, '8.280042e-02'), (201, '5.578709e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '2.649026e-05'), (1, '1.431099e-05'), (2, '8.117711e-05'), (3, '2.119169e-05'), (4, '1.624065e-06'), (5, '1.829690e-06'), (6, '1.123062e-05'), (7, '1.807203e-06'), (8, '1.981399e-06'), (9, '1.253789e-05'), (10, '1.552331e-06'), (11, '1.509303e-06'), (12, '1.139967e-05'), (13, '1.330141e-06'), (14, '1.910165e-06'), (15, '1.193102e-05'), (16, '1.703428e-06'), (17, '1.821925e-06'), (18, '1.186163e-05'), (19, '1.253684e-06'), (20, '1.496160e-06'), (21, '1.176357e-05'), (22, '1.567206e-05'), (23, '4.108498e-04'), (24, '1.566552e-04'), (25, '4.155805e-05'), (26, '7.303632e-06'), (27, '6.824962e-05'), (28, '1.109713e-05'), (29, '3.461919e-04'), (30, '9.780170e-05'), (31, '2.854442e-05'), (32, '6.569894e-06'), (33, '8.994674e-06'), (34, '1.337304e-05'), (35, '9.483000e-06'), (36, '7.929102e-06'), (37, '2.038728e-05'), (38, '2.762415e-06'), (39, '2.144586e-06'), (40, '9.724422e-06'), (41, '5.392270e-06'), (42, '6.113986e-06'), (43, '1.341180e-05'), (44, '6.755244e-06'), (45, '8.285278e-06'), (46, '1.474877e-05'), (47, '7.706942e-06'), (48, '1.105392e-05'), (49, '1.987091e-05'), (50, '1.705764e-05'), (51, '3.289611e-04'), (52, '7.998529e-05'), (53, '4.592828e-05'), (54, '3.268301e-06'), (55, '2.439015e-05'), (56, '5.082863e-06'), (57, '3.183567e-04'), (58, '1.502732e-04'), (59, '4.261698e-05'), (60, '6.030375e-06'), (61, '1.133564e-05'), (62, '1.721529e-05'), (63, '1.410549e-05'), (64, '1.312205e-05'), (65, '2.688383e-05'), (66, '1.170490e-05'), (67, '1.612121e-05'), (68, '1.469477e-05'), (69, '1.222206e-05'), (70, '1.526018e-05'), (71, '1.871487e-05'), (72, '1.020091e-05'), (73, '8.262915e-06'), (74, '2.224879e-05'), (75, '9.663117e-06'), (76, '7.946479e-06'), (77, '2.073765e-05'), (78, '1.972494e-05'), (79, '3.205664e-04'), (80, '6.203102e-05'), (81, '4.111621e-05'), (82, '3.296585e-06'), (83, '3.060093e-05'), (84, '3.657423e-06'), (85, '3.359582e-04'), (86, '1.369540e-04'), (87, '4.850440e-05'), (88, '2.144986e-05'), (89, '2.219292e-05'), (90, '3.394830e-05'), (91, '1.847321e-05'), (92, '2.144310e-05'), (93, '2.427262e-05'), (94, '5.971113e-06'), (95, '6.765629e-06'), (96, '1.973057e-05'), (97, '1.617223e-05'), (98, '1.913675e-05'), (99, '2.339400e-05'), (100, '3.451247e-06'), (101, '4.000136e-06'), (102, '4.597388e-06'), (103, '6.903238e-06'), (104, '1.361825e-05'), (105, '9.902785e-06'), (106, '1.685197e-05'), (107, '3.358821e-04'), (108, '5.509884e-05'), (109, '3.477684e-05'), (110, '3.945650e-06'), (111, '3.738110e-05'), (112, '3.084053e-06'), (113, '3.456128e-04'), (114, '2.059582e-04'), (115, '7.701738e-05'), (116, '1.552717e-05'), (117, '2.836907e-05'), (118, '3.368766e-05'), (119, '1.906533e-05'), (120, '3.639046e-05'), (121, '1.566365e-05'), (122, '2.973554e-05'), (123, '2.476494e-05'), (124, '3.126971e-05'), (125, '3.720569e-05'), (126, '4.416294e-05'), (127, '2.608970e-05'), (128, '3.939872e-05'), (129, '5.231296e-05'), (130, '2.747689e-05'), (131, '3.084069e-05'), (132, '2.906701e-05'), (133, '3.698083e-05'), (134, '2.039024e-05'), (135, '3.127115e-04'), (136, '5.213415e-05'), (137, '3.564569e-05'), (138, '4.744688e-06'), (139, '4.505566e-05'), (140, '3.102183e-06'), (141, '3.219612e-04'), (142, '2.743427e-04'), (143, '6.536551e-05'), (144, '4.355648e-05'), (145, '2.151048e-05'), (146, '5.536123e-05'), (147, '3.334376e-05'), (148, '1.731211e-05'), (149, '3.752091e-05'), (150, '3.086270e-05'), (151, '2.265277e-05'), (152, '4.491017e-05'), (153, '3.532922e-05'), (154, '1.634568e-05'), (155, '4.182242e-05'), (156, '2.776868e-05'), (157, '1.426112e-05'), (158, '5.648647e-05'), (159, '2.619108e-05'), (160, '2.532482e-05'), (161, '2.605968e-05'), (162, '2.558637e-05'), (163, '2.646447e-04'), (164, '9.517319e-05'), (165, '4.914439e-05'), (166, '1.169569e-05'), (167, '8.117560e-05'), (168, '7.770564e-06'), (169, '2.791519e-04'), (170, '2.983387e-04'), (171, '1.397121e-04'), (172, '3.423599e-05'), (173, '1.588308e-05'), (174, '5.908722e-05'), (175, '3.533231e-05'), (176, '1.926252e-05'), (177, '6.793816e-05'), (178, '5.579586e-05'), (179, '1.621463e-05'), (180, '7.422066e-05'), (181, '3.038105e-05'), (182, '1.727456e-05'), (183, '6.163549e-05'), (184, '3.420457e-05'), (185, '1.453217e-05'), (186, '8.089851e-05'), (187, '3.927671e-05'), (188, '1.669873e-05'), (189, '6.491655e-05'), (190, '3.907042e-05'), (191, '2.321088e-04'), (192, '9.802978e-05'), (193, '4.920514e-05'), (194, '4.912705e-05'), (195, '2.017997e-04'), (196, '2.023851e-05'), (197, '1.300650e-04'), (198, '3.175309e-04'), (199, '1.831993e-05'), (200, '1.190697e-04'), (201, '7.095173e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.328361e+00 458.557s| epoch: 50 | grad test: 7.287715e-04,7.287716e-04\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '4.506250e-02'), (1, '1.094676e-01'), (2, '2.155466e-02'), (3, '2.374556e-01'), (4, '2.191023e-01'), (5, '1.012267e+00'), (6, '3.765579e-01'), (7, '1.321379e-01'), (8, '1.831854e-01'), (9, '2.799554e-01'), (10, '2.280263e-01'), (11, '1.046725e-01'), (12, '1.864367e-01'), (13, '1.749581e-01'), (14, '1.894623e-01'), (15, '2.320039e-01'), (16, '3.694204e-01'), (17, '1.429250e-01'), (18, '1.602719e-01'), (19, '1.925211e-01'), (20, '3.407134e-01'), (21, '2.040880e-01'), (22, '3.130083e-01'), (23, '3.293571e-02'), (24, '1.046417e-02'), (25, '1.728786e-01'), (26, '1.121641e-01'), (27, '6.563564e-02'), (28, '1.463371e-01'), (29, '1.104527e-01'), (30, '1.052768e-02'), (31, '1.726708e-01'), (32, '2.275401e-01'), (33, '1.627219e-01'), (34, '7.973587e-01'), (35, '1.496675e-01'), (36, '1.777054e-01'), (37, '1.406899e-01'), (38, '2.270486e-01'), (39, '2.094623e-01'), (40, '1.597336e-01'), (41, '1.750944e-01'), (42, '3.068944e-01'), (43, '1.431686e-01'), (44, '1.738306e-01'), (45, '5.941461e-01'), (46, '1.845679e-01'), (47, '1.255980e-01'), (48, '1.396975e-01'), (49, '2.463891e-01'), (50, '1.694823e-01'), (51, '4.713130e-02'), (52, '1.003199e-02'), (53, '2.038035e-01'), (54, '1.883404e-01'), (55, '2.287178e-02'), (56, '1.477565e-01'), (57, '4.524153e-02'), (58, '1.231326e-02'), (59, '2.224514e-01'), (60, '1.491129e-01'), (61, '1.380228e-01'), (62, '1.474190e-01'), (63, '2.081449e-01'), (64, '1.406231e-01'), (65, '1.085199e-01'), (66, '9.462295e-02'), (67, '1.371810e-01'), (68, '1.478651e-01'), (69, '1.230587e-01'), (70, '2.605948e-01'), (71, '1.939163e-01'), (72, '1.723500e-01'), (73, '2.174079e-01'), (74, '2.346255e-01'), (75, '1.145266e-01'), (76, '2.255136e-01'), (77, '1.289642e-01'), (78, '1.388414e-01'), (79, '9.190834e-02'), (80, '1.042373e-02'), (81, '2.215191e-01'), (82, '1.250807e-01'), (83, '1.787851e-02'), (84, '1.507308e-01'), (85, '5.098215e-02'), (86, '1.065184e-02'), (87, '1.148819e-01'), (88, '1.244785e-01'), (89, '1.364657e-01'), (90, '1.180126e-01'), (91, '1.523092e-01'), (92, '2.229292e-01'), (93, '1.371026e-01'), (94, '1.831632e-01'), (95, '1.755497e-01'), (96, '1.555090e-01'), (97, '1.071867e-01'), (98, '1.563954e-01'), (99, '1.146607e-01'), (100, '1.547410e-01'), (101, '2.396379e-01'), (102, '5.249279e-01'), (103, '1.695667e-01'), (104, '1.699247e-01'), (105, '2.978032e-01'), (106, '1.802007e-01'), (107, '1.144616e-01'), (108, '1.046818e-02'), (109, '1.440617e-01'), (110, '1.741208e-01'), (111, '2.048347e-02'), (112, '1.005858e-01'), (113, '2.935670e-01'), (114, '9.045146e-03'), (115, '1.575594e-01'), (116, '8.819953e-02'), (117, '1.143902e-01'), (118, '1.391619e-01'), (119, '9.264274e-02'), (120, '9.173683e-02'), (121, '2.061811e-01'), (122, '1.049166e-01'), (123, '3.920875e-01'), (124, '9.624742e-02'), (125, '1.288204e-01'), (126, '1.340610e-01'), (127, '1.473688e-01'), (128, '1.304765e-01'), (129, '9.649105e-02'), (130, '9.981173e-02'), (131, '1.069265e-01'), (132, '1.482945e-01'), (133, '2.349219e-01'), (134, '1.900472e-01'), (135, '7.353145e-02'), (136, '1.046152e-02'), (137, '1.855353e-01'), (138, '4.475414e+00'), (139, '1.516543e-02'), (140, '9.083670e-02'), (141, '8.530563e-02'), (142, '6.908196e-03'), (143, '1.289543e-01'), (144, '7.143239e-02'), (145, '8.013299e-02'), (146, '9.795889e-02'), (147, '1.245053e-01'), (148, '1.083028e-01'), (149, '9.761271e-02'), (150, '1.200515e-01'), (151, '1.053910e-01'), (152, '2.925708e-01'), (153, '1.323712e-01'), (154, '5.987878e-02'), (155, '1.302790e-01'), (156, '1.035687e-01'), (157, '1.067688e-01'), (158, '2.288161e-01'), (159, '1.312869e-01'), (160, '4.041913e-01'), (161, '9.400730e-02'), (162, '1.842235e-01'), (163, '9.125884e-02'), (164, '1.164685e-02'), (165, '1.451974e-01'), (166, '6.538487e-02'), (167, '1.166506e-01'), (168, '9.441286e-02'), (169, '5.176128e-02'), (170, '6.061920e-03'), (171, '5.364628e-02'), (172, '1.038738e-01'), (173, '8.482423e-02'), (174, '8.828591e-02'), (175, '1.163033e-01'), (176, '1.155152e-01'), (177, '1.444722e-01'), (178, '9.604969e-02'), (179, '5.953339e-02'), (180, '1.045159e-01'), (181, '9.946981e-02'), (182, '1.165325e-01'), (183, '1.480121e-01'), (184, '6.206461e-02'), (185, '6.834615e-02'), (186, '2.038235e-01'), (187, '9.080094e-02'), (188, '8.992411e-02'), (189, '1.095681e-01'), (190, '1.240467e-01'), (191, '7.115015e-02'), (192, '3.917867e-03'), (193, '2.491625e-01'), (194, '1.004316e-01'), (195, '4.041944e-02'), (196, '2.741959e-01'), (197, '9.103331e-02'), (198, '5.128091e-03'), (199, '1.427905e-01'), (200, '4.768838e-02'), (201, '5.751169e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '3.919673e-05'), (1, '1.694733e-05'), (2, '8.566316e-05'), (3, '2.374834e-05'), (4, '2.946834e-06'), (5, '2.200328e-06'), (6, '1.709694e-05'), (7, '2.460593e-06'), (8, '2.473737e-06'), (9, '1.687224e-05'), (10, '1.822569e-06'), (11, '1.843616e-06'), (12, '1.611797e-05'), (13, '2.115797e-06'), (14, '2.120006e-06'), (15, '1.504031e-05'), (16, '2.851541e-06'), (17, '1.971973e-06'), (18, '1.738273e-05'), (19, '2.235102e-06'), (20, '1.945196e-06'), (21, '1.737752e-05'), (22, '1.879072e-05'), (23, '8.018789e-04'), (24, '1.587054e-04'), (25, '4.753673e-05'), (26, '8.740705e-06'), (27, '9.625689e-05'), (28, '1.465059e-05'), (29, '7.245040e-04'), (30, '1.036306e-04'), (31, '3.534088e-05'), (32, '1.059244e-05'), (33, '9.104110e-06'), (34, '1.812213e-05'), (35, '7.997708e-06'), (36, '8.815355e-06'), (37, '2.095340e-05'), (38, '5.057935e-06'), (39, '3.209064e-06'), (40, '1.942999e-05'), (41, '8.583286e-06'), (42, '8.312711e-06'), (43, '2.250888e-05'), (44, '1.071023e-05'), (45, '1.079173e-05'), (46, '2.058417e-05'), (47, '8.591343e-06'), (48, '1.278989e-05'), (49, '2.795469e-05'), (50, '2.102101e-05'), (51, '6.697538e-04'), (52, '9.103520e-05'), (53, '6.732906e-05'), (54, '5.530647e-06'), (55, '4.699343e-05'), (56, '7.902663e-06'), (57, '6.795055e-04'), (58, '1.548881e-04'), (59, '4.165738e-05'), (60, '1.568564e-05'), (61, '2.733660e-05'), (62, '2.395446e-05'), (63, '1.693662e-05'), (64, '1.371515e-05'), (65, '3.242795e-05'), (66, '1.744614e-05'), (67, '2.103287e-05'), (68, '2.167608e-05'), (69, '1.440411e-05'), (70, '1.188973e-05'), (71, '2.384532e-05'), (72, '1.599937e-05'), (73, '1.172183e-05'), (74, '2.644004e-05'), (75, '1.460607e-05'), (76, '1.073381e-05'), (77, '3.067365e-05'), (78, '2.200555e-05'), (79, '6.136674e-04'), (80, '6.836149e-05'), (81, '4.584009e-05'), (82, '4.522063e-06'), (83, '4.493623e-05'), (84, '5.012525e-06'), (85, '6.035411e-04'), (86, '1.479723e-04'), (87, '3.647870e-05'), (88, '2.415368e-05'), (89, '2.815239e-05'), (90, '3.006600e-05'), (91, '4.175692e-05'), (92, '4.485991e-05'), (93, '3.412701e-05'), (94, '1.383178e-05'), (95, '1.702583e-05'), (96, '2.641994e-05'), (97, '2.356357e-05'), (98, '2.862896e-05'), (99, '2.954466e-05'), (100, '1.040057e-05'), (101, '1.511158e-05'), (102, '8.818869e-06'), (103, '1.190500e-05'), (104, '1.952914e-05'), (105, '1.931865e-05'), (106, '1.796137e-05'), (107, '5.365705e-04'), (108, '5.660987e-05'), (109, '3.489059e-05'), (110, '4.268923e-06'), (111, '3.925312e-05'), (112, '3.846433e-06'), (113, '5.299663e-04'), (114, '2.056675e-04'), (115, '6.728366e-05'), (116, '2.488609e-05'), (117, '3.169248e-05'), (118, '4.878295e-05'), (119, '2.669766e-05'), (120, '4.585299e-05'), (121, '1.566786e-05'), (122, '2.705032e-05'), (123, '3.284703e-05'), (124, '2.954094e-05'), (125, '3.827743e-05'), (126, '4.839335e-05'), (127, '2.434933e-05'), (128, '4.124992e-05'), (129, '4.537815e-05'), (130, '4.548817e-05'), (131, '2.535240e-05'), (132, '2.314995e-05'), (133, '3.627106e-05'), (134, '2.304952e-05'), (135, '4.976530e-04'), (136, '6.150624e-05'), (137, '3.899174e-05'), (138, '5.601803e-06'), (139, '4.893921e-05'), (140, '4.119734e-06'), (141, '4.878334e-04'), (142, '2.851393e-04'), (143, '6.744885e-05'), (144, '4.052942e-05'), (145, '2.311496e-05'), (146, '6.444957e-05'), (147, '4.607304e-05'), (148, '3.079739e-05'), (149, '6.416841e-05'), (150, '4.196540e-05'), (151, '2.957640e-05'), (152, '6.502196e-05'), (153, '4.178023e-05'), (154, '2.882829e-05'), (155, '5.788748e-05'), (156, '3.417570e-05'), (157, '2.282763e-05'), (158, '6.719547e-05'), (159, '3.468684e-05'), (160, '4.017287e-05'), (161, '4.049558e-05'), (162, '2.636437e-05'), (163, '3.630302e-04'), (164, '9.913730e-05'), (165, '5.181308e-05'), (166, '1.298666e-05'), (167, '9.730034e-05'), (168, '9.692547e-06'), (169, '3.712505e-04'), (170, '3.206395e-04'), (171, '1.635683e-04'), (172, '4.564304e-05'), (173, '1.842026e-05'), (174, '7.078583e-05'), (175, '4.592863e-05'), (176, '2.303991e-05'), (177, '7.088213e-05'), (178, '5.196851e-05'), (179, '2.007235e-05'), (180, '8.274250e-05'), (181, '5.221803e-05'), (182, '2.701413e-05'), (183, '8.432451e-05'), (184, '5.238776e-05'), (185, '2.261035e-05'), (186, '7.940370e-05'), (187, '4.562348e-05'), (188, '2.193311e-05'), (189, '7.688424e-05'), (190, '4.203010e-05'), (191, '2.673187e-04'), (192, '9.813163e-05'), (193, '4.860168e-05'), (194, '5.005781e-05'), (195, '2.170843e-04'), (196, '2.039000e-05'), (197, '1.322533e-04'), (198, '3.224422e-04'), (199, '1.740475e-05'), (200, '1.230095e-04'), (201, '7.559636e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.355097e+00 543.730s| epoch: 60 | grad test: -1.261202e-04,-1.261204e-04\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '5.405077e-02'), (1, '9.730653e-02'), (2, '2.781552e-02'), (3, '2.016148e-01'), (4, '2.431767e-01'), (5, '1.966042e-01'), (6, '1.973785e-01'), (7, '1.481676e-01'), (8, '7.168586e-01'), (9, '1.358538e-01'), (10, '1.474428e-01'), (11, '2.189266e-01'), (12, '1.983289e-01'), (13, '1.665871e-01'), (14, '1.610307e-01'), (15, '1.623111e-01'), (16, '2.220404e-01'), (17, '1.732012e-01'), (18, '2.026539e-01'), (19, '1.562777e-01'), (20, '2.175794e-01'), (21, '2.750744e-01'), (22, '2.645149e-01'), (23, '5.104179e-02'), (24, '1.151031e-02'), (25, '2.750176e-01'), (26, '6.369616e-01'), (27, '1.236840e-01'), (28, '1.463831e-01'), (29, '6.661227e-02'), (30, '1.363433e-02'), (31, '1.549521e-01'), (32, '1.140390e-01'), (33, '1.727839e-01'), (34, '3.020446e-01'), (35, '2.191044e-01'), (36, '1.255314e-01'), (37, '1.582482e-01'), (38, '1.158801e-01'), (39, '5.463627e-01'), (40, '1.886910e-01'), (41, '1.657132e-01'), (42, '1.291033e-01'), (43, '1.223294e-01'), (44, '1.551481e-01'), (45, '1.491729e-01'), (46, '4.598705e-01'), (47, '1.344754e-01'), (48, '1.818141e-01'), (49, '2.784134e-01'), (50, '2.334686e-01'), (51, '3.687034e-02'), (52, '1.060722e-02'), (53, '3.319409e-01'), (54, '1.485794e-01'), (55, '2.031245e-02'), (56, '1.448025e-01'), (57, '4.852720e-02'), (58, '1.488432e-02'), (59, '2.721911e-01'), (60, '2.406763e-01'), (61, '1.411420e-01'), (62, '2.298757e-01'), (63, '1.120708e-01'), (64, '1.355940e-01'), (65, '6.148660e-01'), (66, '2.036010e-01'), (67, '1.850940e-01'), (68, '2.574826e-01'), (69, '2.601581e-01'), (70, '1.411636e-01'), (71, '1.446230e-01'), (72, '1.469333e-01'), (73, '1.451520e-01'), (74, '1.094589e-01'), (75, '4.920849e-01'), (76, '1.859923e-01'), (77, '1.283792e-01'), (78, '2.104200e-01'), (79, '9.013477e-02'), (80, '1.082885e-02'), (81, '1.759483e-01'), (82, '8.917321e-02'), (83, '1.424338e-02'), (84, '6.968299e-02'), (85, '6.137531e-02'), (86, '1.381247e-02'), (87, '2.057676e-01'), (88, '2.205818e-01'), (89, '1.005244e-01'), (90, '2.123542e-01'), (91, '1.464684e-01'), (92, '1.377856e-01'), (93, '1.633849e-01'), (94, '1.521510e-01'), (95, '3.876845e-01'), (96, '8.865409e-01'), (97, '1.122584e-01'), (98, '1.174927e-01'), (99, '8.067091e-02'), (100, '1.317882e-01'), (101, '1.084915e-01'), (102, '2.987902e-01'), (103, '1.257980e-01'), (104, '1.135105e-01'), (105, '2.386736e-01'), (106, '1.771119e-01'), (107, '1.268075e-01'), (108, '1.053068e-02'), (109, '4.142938e-01'), (110, '1.175009e-01'), (111, '9.343016e-03'), (112, '1.156010e-01'), (113, '3.032536e-01'), (114, '9.823882e-03'), (115, '2.475207e-01'), (116, '1.748093e-01'), (117, '2.629102e-01'), (118, '1.612571e-01'), (119, '9.771980e-02'), (120, '3.363510e-01'), (121, '1.280553e-01'), (122, '2.550223e-01'), (123, '1.821936e-01'), (124, '1.849485e-01'), (125, '2.130095e-01'), (126, '2.854044e-01'), (127, '9.779761e-01'), (128, '1.311490e-01'), (129, '1.682809e-01'), (130, '9.266321e-02'), (131, '1.336811e-01'), (132, '1.470890e-01'), (133, '1.384194e-01'), (134, '2.317935e-01'), (135, '5.532982e-02'), (136, '1.223243e-02'), (137, '1.420534e-01'), (138, '6.289961e-02'), (139, '1.666673e-02'), (140, '7.785577e-02'), (141, '2.312190e-01'), (142, '7.496898e-03'), (143, '2.330121e-01'), (144, '8.036048e-01'), (145, '1.094447e-01'), (146, '1.017251e-01'), (147, '1.091445e-01'), (148, '9.597468e-02'), (149, '1.196603e-01'), (150, '1.461561e-01'), (151, '1.013994e-01'), (152, '8.710707e-02'), (153, '9.596650e-02'), (154, '1.614748e-01'), (155, '9.148622e-02'), (156, '1.731918e-01'), (157, '8.566464e-02'), (158, '7.594816e-02'), (159, '9.727834e-02'), (160, '1.137934e-01'), (161, '1.721082e-01'), (162, '2.218857e-01'), (163, '5.820441e-02'), (164, '8.591541e-03'), (165, '1.837111e-01'), (166, '7.802213e-02'), (167, '4.694507e-02'), (168, '7.262826e-02'), (169, '6.876455e-02'), (170, '6.596068e-03'), (171, '8.369237e-02'), (172, '1.315692e-01'), (173, '8.414259e-02'), (174, '1.125621e-01'), (175, '1.450877e-01'), (176, '1.106364e-01'), (177, '1.514452e-01'), (178, '8.398431e-02'), (179, '6.052765e-02'), (180, '1.377724e-01'), (181, '8.030958e-02'), (182, '8.307797e-02'), (183, '7.550832e-02'), (184, '8.420294e-02'), (185, '5.258900e-02'), (186, '1.266514e-01'), (187, '1.011399e-01'), (188, '9.219284e-02'), (189, '1.302578e-01'), (190, '2.484365e-01'), (191, '5.635053e-02'), (192, '4.121170e-03'), (193, '8.926502e-01'), (194, '1.627781e-01'), (195, '4.216113e-02'), (196, '9.235963e-02'), (197, '6.443588e-02'), (198, '5.188006e-03'), (199, '1.780879e-01'), (200, '4.017323e-02'), (201, '6.500650e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '3.986168e-05'), (1, '1.776067e-05'), (2, '8.491027e-05'), (3, '2.575331e-05'), (4, '2.137036e-06'), (5, '2.087836e-06'), (6, '1.821170e-05'), (7, '2.188506e-06'), (8, '1.924277e-06'), (9, '1.590764e-05'), (10, '1.706278e-06'), (11, '1.665157e-06'), (12, '1.642241e-05'), (13, '1.906014e-06'), (14, '1.714857e-06'), (15, '1.605632e-05'), (16, '2.464655e-06'), (17, '1.922359e-06'), (18, '1.522127e-05'), (19, '1.891644e-06'), (20, '1.730337e-06'), (21, '1.591884e-05'), (22, '1.910400e-05'), (23, '7.856817e-04'), (24, '1.713158e-04'), (25, '4.469220e-05'), (26, '8.072706e-06'), (27, '8.919568e-05'), (28, '1.396207e-05'), (29, '6.974445e-04'), (30, '1.215162e-04'), (31, '3.681994e-05'), (32, '6.169318e-06'), (33, '7.355012e-06'), (34, '2.227455e-05'), (35, '8.454444e-06'), (36, '6.821651e-06'), (37, '2.356962e-05'), (38, '2.700886e-06'), (39, '2.185800e-06'), (40, '1.434750e-05'), (41, '6.569340e-06'), (42, '7.342007e-06'), (43, '1.612285e-05'), (44, '7.531476e-06'), (45, '7.500423e-06'), (46, '2.162871e-05'), (47, '8.481526e-06'), (48, '1.043320e-05'), (49, '2.924957e-05'), (50, '2.113352e-05'), (51, '6.338425e-04'), (52, '9.145332e-05'), (53, '3.953571e-05'), (54, '4.775909e-06'), (55, '4.170218e-05'), (56, '6.549362e-06'), (57, '6.271152e-04'), (58, '1.715650e-04'), (59, '5.557765e-05'), (60, '1.376060e-05'), (61, '1.942927e-05'), (62, '3.005169e-05'), (63, '1.030062e-05'), (64, '1.473760e-05'), (65, '3.089296e-05'), (66, '9.696124e-06'), (67, '2.326259e-05'), (68, '2.559591e-05'), (69, '1.352034e-05'), (70, '1.746535e-05'), (71, '2.162629e-05'), (72, '1.147525e-05'), (73, '1.259927e-05'), (74, '2.330109e-05'), (75, '9.495950e-06'), (76, '8.866795e-06'), (77, '2.204697e-05'), (78, '2.242475e-05'), (79, '5.919007e-04'), (80, '6.578316e-05'), (81, '3.127769e-05'), (82, '4.100633e-06'), (83, '4.027472e-05'), (84, '4.514399e-06'), (85, '5.844022e-04'), (86, '1.520236e-04'), (87, '6.698328e-05'), (88, '2.320806e-05'), (89, '2.629659e-05'), (90, '2.821364e-05'), (91, '3.200738e-05'), (92, '3.519305e-05'), (93, '3.542719e-05'), (94, '1.093500e-05'), (95, '9.939858e-06'), (96, '3.396044e-05'), (97, '2.059194e-05'), (98, '2.167339e-05'), (99, '3.066078e-05'), (100, '3.539611e-06'), (101, '9.383002e-06'), (102, '7.242265e-06'), (103, '7.925027e-06'), (104, '1.534654e-05'), (105, '1.056154e-05'), (106, '1.832122e-05'), (107, '5.268295e-04'), (108, '6.186061e-05'), (109, '3.505388e-05'), (110, '4.548615e-06'), (111, '4.381860e-05'), (112, '3.353672e-06'), (113, '5.589491e-04'), (114, '2.281124e-04'), (115, '7.065977e-05'), (116, '2.359246e-05'), (117, '3.327230e-05'), (118, '3.393017e-05'), (119, '2.532864e-05'), (120, '5.255486e-05'), (121, '1.662842e-05'), (122, '3.212598e-05'), (123, '3.798267e-05'), (124, '3.239833e-05'), (125, '4.190610e-05'), (126, '5.539152e-05'), (127, '3.103118e-05'), (128, '3.893549e-05'), (129, '5.827032e-05'), (130, '3.408385e-05'), (131, '2.568632e-05'), (132, '2.624040e-05'), (133, '4.197250e-05'), (134, '2.342700e-05'), (135, '4.843832e-04'), (136, '6.612083e-05'), (137, '4.272693e-05'), (138, '5.786933e-06'), (139, '6.307280e-05'), (140, '4.265766e-06'), (141, '5.239901e-04'), (142, '2.976506e-04'), (143, '8.884507e-05'), (144, '4.599651e-05'), (145, '3.510376e-05'), (146, '6.020386e-05'), (147, '4.634286e-05'), (148, '3.119680e-05'), (149, '6.730236e-05'), (150, '5.435543e-05'), (151, '4.012657e-05'), (152, '6.178043e-05'), (153, '4.774048e-05'), (154, '2.912647e-05'), (155, '4.579518e-05'), (156, '3.432173e-05'), (157, '2.481218e-05'), (158, '5.995590e-05'), (159, '4.971675e-05'), (160, '4.126926e-05'), (161, '6.044799e-05'), (162, '2.769143e-05'), (163, '3.678300e-04'), (164, '1.044290e-04'), (165, '5.248523e-05'), (166, '1.362431e-05'), (167, '1.018798e-04'), (168, '9.387652e-06'), (169, '3.488708e-04'), (170, '3.492384e-04'), (171, '1.904906e-04'), (172, '5.544310e-05'), (173, '1.769066e-05'), (174, '7.676433e-05'), (175, '4.925510e-05'), (176, '2.097031e-05'), (177, '7.642110e-05'), (178, '5.045193e-05'), (179, '1.882528e-05'), (180, '7.591595e-05'), (181, '3.951792e-05'), (182, '1.754554e-05'), (183, '7.261911e-05'), (184, '4.462541e-05'), (185, '2.069687e-05'), (186, '9.041963e-05'), (187, '5.652642e-05'), (188, '2.679630e-05'), (189, '7.418364e-05'), (190, '4.376461e-05'), (191, '2.614586e-04'), (192, '1.057257e-04'), (193, '5.295742e-05'), (194, '5.214824e-05'), (195, '2.333721e-04'), (196, '2.041310e-05'), (197, '1.393377e-04'), (198, '3.221250e-04'), (199, '2.114237e-05'), (200, '1.149256e-04'), (201, '6.749729e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.335825e+00 634.198s| epoch: 70 | grad test: -9.247877e-05,-9.247884e-05\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.374124e-02'), (1, '2.607521e-01'), (2, '1.965936e-02'), (3, '1.800270e-01'), (4, '3.673431e-01'), (5, '1.393955e+00'), (6, '1.200481e-01'), (7, '1.531556e-01'), (8, '2.048492e-01'), (9, '1.315232e-01'), (10, '9.989918e-02'), (11, '5.628192e-01'), (12, '2.929027e-01'), (13, '1.194637e-01'), (14, '1.356361e-01'), (15, '1.544265e-01'), (16, '2.302657e-01'), (17, '2.088112e-01'), (18, '1.307579e-01'), (19, '1.380862e-01'), (20, '1.645178e-01'), (21, '3.203258e-01'), (22, '1.560228e-01'), (23, '1.255639e-01'), (24, '1.083817e-02'), (25, '1.260819e+00'), (26, '1.494826e-01'), (27, '5.973728e-02'), (28, '1.090642e-01'), (29, '2.552278e-01'), (30, '1.151623e-02'), (31, '7.291106e-01'), (32, '1.593274e-01'), (33, '2.265503e-01'), (34, '1.392437e-01'), (35, '2.655073e-01'), (36, '1.620674e-01'), (37, '1.702932e-01'), (38, '9.370716e-02'), (39, '6.271033e-02'), (40, '1.447104e-01'), (41, '1.078394e-01'), (42, '2.625592e-01'), (43, '1.107489e-01'), (44, '1.101177e-01'), (45, '3.689323e-01'), (46, '1.145084e-01'), (47, '1.506947e-01'), (48, '1.417540e-01'), (49, '2.276005e-01'), (50, '1.445750e-01'), (51, '3.659950e-02'), (52, '1.005787e-02'), (53, '2.720220e-01'), (54, '1.078730e-01'), (55, '2.627795e-02'), (56, '1.036914e-01'), (57, '4.890887e-02'), (58, '9.782557e-03'), (59, '1.442740e-01'), (60, '4.012924e-01'), (61, '1.816258e-01'), (62, '1.483468e-01'), (63, '8.679479e-02'), (64, '1.028341e-01'), (65, '2.034548e-01'), (66, '1.332161e-01'), (67, '1.459120e+00'), (68, '1.121332e-01'), (69, '1.478967e-01'), (70, '1.707443e-01'), (71, '1.173850e-01'), (72, '1.396978e-01'), (73, '1.543763e-01'), (74, '2.489887e-01'), (75, '9.696009e-02'), (76, '1.483404e-01'), (77, '1.849887e-01'), (78, '1.604654e-01'), (79, '9.275323e-02'), (80, '9.708731e-03'), (81, '1.479126e-01'), (82, '8.993716e-02'), (83, '1.150399e-02'), (84, '1.102640e-01'), (85, '6.939284e-02'), (86, '1.315735e-02'), (87, '1.280459e-01'), (88, '1.095175e-01'), (89, '1.438446e-01'), (90, '1.483357e-01'), (91, '4.773548e-01'), (92, '1.082327e-01'), (93, '1.760342e-01'), (94, '1.796415e-01'), (95, '1.392911e-01'), (96, '1.231718e-01'), (97, '1.431666e-01'), (98, '1.498093e-01'), (99, '1.610698e-01'), (100, '1.168017e-01'), (101, '1.308013e-01'), (102, '2.735188e-01'), (103, '8.106822e-02'), (104, '8.317813e-02'), (105, '1.821809e-01'), (106, '2.053553e-01'), (107, '5.221613e-02'), (108, '1.048402e-02'), (109, '1.492729e-01'), (110, '1.146643e-01'), (111, '1.635601e-02'), (112, '8.110491e-02'), (113, '7.767685e-02'), (114, '8.667137e-03'), (115, '1.576451e-01'), (116, '7.337520e-02'), (117, '7.218543e-02'), (118, '2.141173e-01'), (119, '1.332506e-01'), (120, '1.688743e-01'), (121, '1.134393e-01'), (122, '1.078631e-01'), (123, '1.260206e-01'), (124, '5.572208e-01'), (125, '7.862296e-02'), (126, '8.551630e-02'), (127, '8.830968e-02'), (128, '1.490459e-01'), (129, '7.481837e-02'), (130, '9.142758e-02'), (131, '1.054710e-01'), (132, '8.228501e-02'), (133, '1.664248e-01'), (134, '1.547852e-01'), (135, '8.863274e-01'), (136, '1.105916e-02'), (137, '1.553308e-01'), (138, '1.061409e-01'), (139, '2.281260e-02'), (140, '8.625022e-02'), (141, '9.341022e-02'), (142, '7.004714e-03'), (143, '3.746937e-01'), (144, '9.262763e-02'), (145, '7.381957e-02'), (146, '1.053021e-01'), (147, '9.681590e-02'), (148, '8.981347e-02'), (149, '1.075511e-01'), (150, '1.877345e-01'), (151, '1.022431e-01'), (152, '8.904436e-02'), (153, '8.258773e-02'), (154, '1.199122e-01'), (155, '1.333223e-01'), (156, '1.277242e-01'), (157, '8.694032e-02'), (158, '2.611874e-01'), (159, '9.556758e-02'), (160, '1.262693e-01'), (161, '1.158084e-01'), (162, '9.896553e+00'), (163, '5.106570e-02'), (164, '7.939182e-03'), (165, '1.845206e-01'), (166, '7.786132e-02'), (167, '5.773696e-02'), (168, '3.642021e+00'), (169, '6.444501e-02'), (170, '5.878537e-03'), (171, '6.464920e-02'), (172, '8.861648e-02'), (173, '6.231585e-02'), (174, '8.838673e-02'), (175, '7.834832e-02'), (176, '7.479268e-02'), (177, '2.193013e-01'), (178, '8.011178e-02'), (179, '5.886890e-02'), (180, '2.351187e-01'), (181, '5.277885e-01'), (182, '7.998081e-02'), (183, '1.011332e-01'), (184, '6.525516e-02'), (185, '6.124992e-01'), (186, '4.055514e-01'), (187, '8.881241e-02'), (188, '8.958118e-02'), (189, '1.131967e-01'), (190, '1.267816e-01'), (191, '1.103839e-01'), (192, '3.998221e-03'), (193, '2.109714e-01'), (194, '1.170980e-01'), (195, '7.164059e-02'), (196, '6.619356e-02'), (197, '7.827879e-02'), (198, '5.049357e-03'), (199, '1.289750e-01'), (200, '3.140329e-02'), (201, '5.561716e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '2.379753e-05'), (1, '1.436397e-05'), (2, '7.268063e-05'), (3, '1.529259e-05'), (4, '1.935946e-06'), (5, '1.725218e-06'), (6, '9.286807e-06'), (7, '2.140885e-06'), (8, '2.047424e-06'), (9, '9.549783e-06'), (10, '1.383172e-06'), (11, '1.469843e-06'), (12, '9.363526e-06'), (13, '1.449952e-06'), (14, '1.573662e-06'), (15, '9.971341e-06'), (16, '1.664631e-06'), (17, '1.670284e-06'), (18, '8.794390e-06'), (19, '1.398468e-06'), (20, '1.481199e-06'), (21, '9.039221e-06'), (22, '1.316044e-05'), (23, '4.306222e-04'), (24, '1.585939e-04'), (25, '4.268162e-05'), (26, '6.306022e-06'), (27, '5.299028e-05'), (28, '1.039607e-05'), (29, '3.887751e-04'), (30, '9.321301e-05'), (31, '2.865362e-05'), (32, '4.714403e-06'), (33, '6.411362e-06'), (34, '1.359219e-05'), (35, '7.160361e-06'), (36, '5.711879e-06'), (37, '1.646806e-05'), (38, '2.752621e-06'), (39, '1.904697e-06'), (40, '1.034410e-05'), (41, '7.079126e-06'), (42, '5.759319e-06'), (43, '1.230430e-05'), (44, '6.086041e-06'), (45, '5.387198e-06'), (46, '1.428392e-05'), (47, '6.165139e-06'), (48, '7.014231e-06'), (49, '1.112285e-05'), (50, '1.468326e-05'), (51, '3.799536e-04'), (52, '8.342545e-05'), (53, '3.979257e-05'), (54, '3.071363e-06'), (55, '2.469285e-05'), (56, '4.981450e-06'), (57, '3.769035e-04'), (58, '1.404107e-04'), (59, '3.853386e-05'), (60, '7.053648e-06'), (61, '1.310354e-05'), (62, '1.292962e-05'), (63, '8.892263e-06'), (64, '1.126623e-05'), (65, '1.884705e-05'), (66, '1.004978e-05'), (67, '1.818553e-05'), (68, '1.272644e-05'), (69, '1.212961e-05'), (70, '1.251928e-05'), (71, '1.375939e-05'), (72, '8.453853e-06'), (73, '8.151035e-06'), (74, '1.655817e-05'), (75, '7.738013e-06'), (76, '6.319598e-06'), (77, '1.753852e-05'), (78, '1.821131e-05'), (79, '3.564594e-04'), (80, '5.798949e-05'), (81, '3.022820e-05'), (82, '2.758157e-06'), (83, '2.065313e-05'), (84, '3.447847e-06'), (85, '3.597212e-04'), (86, '1.601930e-04'), (87, '4.653819e-05'), (88, '2.062841e-05'), (89, '1.863740e-05'), (90, '2.731314e-05'), (91, '1.778386e-05'), (92, '2.360534e-05'), (93, '2.210459e-05'), (94, '1.113038e-05'), (95, '1.127568e-05'), (96, '1.958389e-05'), (97, '2.081981e-05'), (98, '1.962372e-05'), (99, '2.636163e-05'), (100, '4.607846e-06'), (101, '7.514264e-06'), (102, '6.372382e-06'), (103, '5.081366e-06'), (104, '6.912524e-06'), (105, '8.682592e-06'), (106, '1.674937e-05'), (107, '3.516065e-04'), (108, '5.425040e-05'), (109, '3.710234e-05'), (110, '3.425774e-06'), (111, '2.801269e-05'), (112, '3.035543e-06'), (113, '3.503055e-04'), (114, '2.113298e-04'), (115, '9.474019e-05'), (116, '1.759770e-05'), (117, '2.264466e-05'), (118, '3.343166e-05'), (119, '2.057840e-05'), (120, '4.716642e-05'), (121, '1.892018e-05'), (122, '2.081416e-05'), (123, '2.693959e-05'), (124, '2.937741e-05'), (125, '2.188440e-05'), (126, '4.146130e-05'), (127, '2.073296e-05'), (128, '3.338271e-05'), (129, '4.529871e-05'), (130, '2.742536e-05'), (131, '1.880630e-05'), (132, '1.812333e-05'), (133, '2.307175e-05'), (134, '2.208661e-05'), (135, '3.536155e-04'), (136, '5.797502e-05'), (137, '4.259363e-05'), (138, '4.710647e-06'), (139, '3.867403e-05'), (140, '3.359333e-06'), (141, '3.726514e-04'), (142, '2.610291e-04'), (143, '7.658025e-05'), (144, '3.414600e-05'), (145, '1.467555e-05'), (146, '4.940367e-05'), (147, '3.581884e-05'), (148, '1.698762e-05'), (149, '5.565259e-05'), (150, '2.982484e-05'), (151, '1.766985e-05'), (152, '4.750047e-05'), (153, '5.113416e-05'), (154, '1.763992e-05'), (155, '4.870139e-05'), (156, '2.546045e-05'), (157, '1.432710e-05'), (158, '5.303612e-05'), (159, '2.567020e-05'), (160, '2.454520e-05'), (161, '3.640534e-05'), (162, '2.575937e-05'), (163, '2.883321e-04'), (164, '9.641447e-05'), (165, '4.941356e-05'), (166, '1.140487e-05'), (167, '7.980815e-05'), (168, '7.783299e-06'), (169, '2.728597e-04'), (170, '2.934003e-04'), (171, '1.525122e-04'), (172, '5.194650e-05'), (173, '1.461462e-05'), (174, '6.392795e-05'), (175, '3.205059e-05'), (176, '1.815863e-05'), (177, '5.801626e-05'), (178, '6.380490e-05'), (179, '1.691468e-05'), (180, '6.038391e-05'), (181, '3.885957e-05'), (182, '1.804325e-05'), (183, '6.092978e-05'), (184, '3.040672e-05'), (185, '1.639453e-05'), (186, '7.390129e-05'), (187, '3.935355e-05'), (188, '1.672297e-05'), (189, '6.260890e-05'), (190, '3.965521e-05'), (191, '2.158301e-04'), (192, '9.836642e-05'), (193, '4.880312e-05'), (194, '4.848957e-05'), (195, '1.942902e-04'), (196, '2.004587e-05'), (197, '1.083780e-04'), (198, '3.132611e-04'), (199, '2.326279e-05'), (200, '1.124669e-04'), (201, '6.848561e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.324641e+00 713.717s| epoch: 80 | grad test: -4.688076e-05,-4.688076e-05\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '2.734021e-02'), (1, '9.403261e-02'), (2, '1.857627e-02'), (3, '1.586556e-01'), (4, '3.220927e-01'), (5, '3.088643e-01'), (6, '1.710179e-01'), (7, '2.151168e-01'), (8, '1.652115e-01'), (9, '1.296487e-01'), (10, '9.784926e-02'), (11, '1.132806e-01'), (12, '1.461353e-01'), (13, '2.269275e-01'), (14, '1.312212e-01'), (15, '2.705553e-01'), (16, '2.334720e-01'), (17, '2.150727e-01'), (18, '1.419268e-01'), (19, '1.779409e-01'), (20, '3.026669e-01'), (21, '1.164435e-01'), (22, '2.915746e-01'), (23, '6.865575e-02'), (24, '1.001669e-02'), (25, '2.459611e-01'), (26, '1.005222e-01'), (27, '5.428676e-02'), (28, '1.366035e-01'), (29, '8.164529e-02'), (30, '1.210157e-02'), (31, '4.409629e-01'), (32, '2.305216e-01'), (33, '1.219039e-01'), (34, '1.519917e-01'), (35, '2.671873e-01'), (36, '1.720379e-01'), (37, '1.152146e-01'), (38, '1.260097e-01'), (39, '1.168919e-01'), (40, '9.786428e-01'), (41, '2.136551e-01'), (42, '3.133288e-01'), (43, '1.681621e-01'), (44, '1.141170e-01'), (45, '1.070247e-01'), (46, '1.060873e-01'), (47, '2.680493e-01'), (48, '4.432763e-01'), (49, '1.438155e-01'), (50, '2.354411e-01'), (51, '3.316032e-02'), (52, '9.431657e-03'), (53, '2.817696e-01'), (54, '5.829888e-02'), (55, '1.785816e-02'), (56, '1.374048e-01'), (57, '4.149050e-02'), (58, '1.029209e-02'), (59, '1.725498e-01'), (60, '1.300731e-01'), (61, '1.151908e-01'), (62, '1.559054e-01'), (63, '9.916063e-02'), (64, '1.302254e-01'), (65, '1.263117e-01'), (66, '1.829865e-01'), (67, '1.744377e-01'), (68, '1.282120e-01'), (69, '4.289838e-01'), (70, '1.404563e-01'), (71, '1.211262e-01'), (72, '1.434961e+00'), (73, '1.270898e-01'), (74, '1.238943e-01'), (75, '1.456637e-01'), (76, '1.128604e-01'), (77, '1.488443e-01'), (78, '1.899235e-01'), (79, '8.489630e-02'), (80, '1.006583e-02'), (81, '1.795680e-01'), (82, '7.794306e-02'), (83, '1.266592e-02'), (84, '6.930766e-02'), (85, '9.481893e-02'), (86, '1.129571e-02'), (87, '1.888696e-01'), (88, '1.211252e-01'), (89, '9.675400e-02'), (90, '1.147255e-01'), (91, '1.027318e-01'), (92, '1.470808e-01'), (93, '1.864019e-01'), (94, '1.400288e-01'), (95, '1.866465e-01'), (96, '1.570101e-01'), (97, '1.202419e-01'), (98, '9.364900e-02'), (99, '1.505498e-01'), (100, '1.033653e-01'), (101, '8.247945e-02'), (102, '2.417499e-01'), (103, '1.947859e-01'), (104, '1.369349e-01'), (105, '2.462811e-01'), (106, '1.368753e-01'), (107, '6.811044e-02'), (108, '1.411911e-02'), (109, '6.509191e-01'), (110, '1.146474e-01'), (111, '1.553311e-02'), (112, '1.404261e-01'), (113, '7.086607e-02'), (114, '8.816671e-03'), (115, '3.836523e-01'), (116, '8.344379e-02'), (117, '1.621641e-01'), (118, '2.191583e-01'), (119, '2.000021e-01'), (120, '9.413087e-02'), (121, '1.770105e-01'), (122, '1.228205e-01'), (123, '1.136659e-01'), (124, '1.142871e-01'), (125, '1.147260e-01'), (126, '1.814782e-01'), (127, '9.456480e-02'), (128, '2.675182e-01'), (129, '9.599350e-02'), (130, '8.384529e-02'), (131, '7.525915e-02'), (132, '9.564177e-02'), (133, '1.313784e-01'), (134, '1.417638e-01'), (135, '1.245973e-01'), (136, '9.765825e-03'), (137, '1.928944e-01'), (138, '2.523381e-01'), (139, '1.266684e-02'), (140, '1.621345e-01'), (141, '1.338452e-01'), (142, '7.140810e-03'), (143, '2.029959e-01'), (144, '7.448193e-02'), (145, '5.556437e-02'), (146, '8.629055e-02'), (147, '1.216260e-01'), (148, '8.982336e-02'), (149, '1.143892e-01'), (150, '9.117375e-02'), (151, '1.055354e-01'), (152, '9.603447e-02'), (153, '1.011969e-01'), (154, '9.460340e-02'), (155, '9.090384e-02'), (156, '9.355766e-02'), (157, '7.273265e-02'), (158, '9.303374e-02'), (159, '1.522771e-01'), (160, '1.395787e-01'), (161, '1.194874e-01'), (162, '3.200450e-01'), (163, '5.007069e-02'), (164, '7.690183e-03'), (165, '2.249828e-01'), (166, '3.507361e-01'), (167, '4.561499e-02'), (168, '2.028745e-01'), (169, '6.292342e-02'), (170, '5.781873e-03'), (171, '4.859146e-02'), (172, '1.284244e-01'), (173, '8.534995e-02'), (174, '7.948805e-02'), (175, '1.159678e-01'), (176, '5.995170e-02'), (177, '7.538077e-02'), (178, '7.700190e-02'), (179, '7.488205e-02'), (180, '7.539250e-02'), (181, '9.490341e-02'), (182, '7.355654e-02'), (183, '2.153708e-01'), (184, '6.955498e-02'), (185, '5.074548e-02'), (186, '1.508606e-01'), (187, '1.042567e-01'), (188, '6.176119e-02'), (189, '1.289767e-01'), (190, '1.364883e-01'), (191, '8.761050e-02'), (192, '4.010258e-03'), (193, '1.986958e-01'), (194, '9.267327e-02'), (195, '7.014126e-02'), (196, '1.380525e-01'), (197, '5.072025e-02'), (198, '5.140547e-03'), (199, '1.508601e-01'), (200, '4.263313e-02'), (201, '5.705097e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '2.875159e-05'), (1, '1.483620e-05'), (2, '7.616010e-05'), (3, '1.946095e-05'), (4, '1.717589e-06'), (5, '1.795304e-06'), (6, '1.070294e-05'), (7, '1.773732e-06'), (8, '2.008426e-06'), (9, '1.148461e-05'), (10, '1.412590e-06'), (11, '1.595053e-06'), (12, '1.094775e-05'), (13, '1.360933e-06'), (14, '1.362410e-06'), (15, '1.028476e-05'), (16, '1.920879e-06'), (17, '1.811718e-06'), (18, '1.107145e-05'), (19, '1.618647e-06'), (20, '1.637492e-06'), (21, '1.092046e-05'), (22, '1.308567e-05'), (23, '5.418607e-04'), (24, '1.511052e-04'), (25, '4.322661e-05'), (26, '6.334619e-06'), (27, '6.548444e-05'), (28, '1.134378e-05'), (29, '4.813549e-04'), (30, '1.115888e-04'), (31, '3.535810e-05'), (32, '5.227836e-06'), (33, '6.830456e-06'), (34, '1.488321e-05'), (35, '1.276970e-05'), (36, '9.687058e-06'), (37, '2.013576e-05'), (38, '5.743585e-06'), (39, '3.698265e-06'), (40, '1.282128e-05'), (41, '9.037654e-06'), (42, '8.325257e-06'), (43, '1.515103e-05'), (44, '5.180468e-06'), (45, '8.625402e-06'), (46, '1.601632e-05'), (47, '8.143896e-06'), (48, '9.302894e-06'), (49, '2.150719e-05'), (50, '1.676492e-05'), (51, '4.930167e-04'), (52, '7.892695e-05'), (53, '2.787853e-05'), (54, '2.952532e-06'), (55, '2.700801e-05'), (56, '5.153957e-06'), (57, '4.722840e-04'), (58, '1.514827e-04'), (59, '5.040977e-05'), (60, '9.991851e-06'), (61, '1.507248e-05'), (62, '1.727777e-05'), (63, '9.353172e-06'), (64, '1.092148e-05'), (65, '2.339655e-05'), (66, '1.205955e-05'), (67, '2.195758e-05'), (68, '1.956667e-05'), (69, '1.454372e-05'), (70, '1.617365e-05'), (71, '1.895222e-05'), (72, '1.647571e-05'), (73, '1.251431e-05'), (74, '2.876030e-05'), (75, '1.044720e-05'), (76, '8.471038e-06'), (77, '2.016838e-05'), (78, '1.785848e-05'), (79, '4.554037e-04'), (80, '6.263880e-05'), (81, '3.043415e-05'), (82, '3.051911e-06'), (83, '2.880477e-05'), (84, '3.681796e-06'), (85, '4.773571e-04'), (86, '1.825092e-04'), (87, '3.942500e-05'), (88, '2.624092e-05'), (89, '2.721458e-05'), (90, '2.813224e-05'), (91, '3.088951e-05'), (92, '3.865881e-05'), (93, '2.978296e-05'), (94, '1.497884e-05'), (95, '1.676747e-05'), (96, '3.160655e-05'), (97, '2.365448e-05'), (98, '2.515935e-05'), (99, '3.340165e-05'), (100, '4.875048e-06'), (101, '5.562055e-06'), (102, '6.582661e-06'), (103, '9.867973e-06'), (104, '1.552724e-05'), (105, '1.002688e-05'), (106, '1.858595e-05'), (107, '4.261426e-04'), (108, '5.366208e-05'), (109, '3.484221e-05'), (110, '3.514032e-06'), (111, '2.916232e-05'), (112, '3.134277e-06'), (113, '4.396650e-04'), (114, '2.191425e-04'), (115, '6.536514e-05'), (116, '1.832959e-05'), (117, '2.701914e-05'), (118, '3.282715e-05'), (119, '2.622684e-05'), (120, '4.508621e-05'), (121, '1.608632e-05'), (122, '2.075494e-05'), (123, '2.594848e-05'), (124, '2.680977e-05'), (125, '1.914190e-05'), (126, '3.814574e-05'), (127, '1.772314e-05'), (128, '3.902672e-05'), (129, '5.252414e-05'), (130, '3.550664e-05'), (131, '1.964234e-05'), (132, '1.699241e-05'), (133, '3.002780e-05'), (134, '2.449272e-05'), (135, '4.632034e-04'), (136, '5.190795e-05'), (137, '4.201347e-05'), (138, '4.900985e-06'), (139, '5.055313e-05'), (140, '3.711892e-06'), (141, '5.391245e-04'), (142, '2.601379e-04'), (143, '7.988328e-05'), (144, '3.559055e-05'), (145, '1.660033e-05'), (146, '4.968718e-05'), (147, '3.914713e-05'), (148, '1.939342e-05'), (149, '5.967021e-05'), (150, '3.465944e-05'), (151, '2.052672e-05'), (152, '6.016238e-05'), (153, '5.405790e-05'), (154, '2.097542e-05'), (155, '5.481843e-05'), (156, '2.985867e-05'), (157, '1.832985e-05'), (158, '6.509546e-05'), (159, '3.200807e-05'), (160, '2.563058e-05'), (161, '3.491539e-05'), (162, '2.525975e-05'), (163, '3.726729e-04'), (164, '9.557775e-05'), (165, '4.818521e-05'), (166, '1.275405e-05'), (167, '1.032355e-04'), (168, '8.797247e-06'), (169, '4.489658e-04'), (170, '2.785288e-04'), (171, '1.393360e-04'), (172, '3.531807e-05'), (173, '1.329272e-05'), (174, '6.419409e-05'), (175, '3.713890e-05'), (176, '1.648598e-05'), (177, '6.765999e-05'), (178, '5.333267e-05'), (179, '1.487465e-05'), (180, '8.676069e-05'), (181, '4.011498e-05'), (182, '1.937806e-05'), (183, '9.038648e-05'), (184, '4.109920e-05'), (185, '1.830879e-05'), (186, '9.340266e-05'), (187, '4.542146e-05'), (188, '1.616383e-05'), (189, '8.034972e-05'), (190, '4.135713e-05'), (191, '3.027968e-04'), (192, '1.022925e-04'), (193, '4.603730e-05'), (194, '4.803609e-05'), (195, '2.034322e-04'), (196, '1.986448e-05'), (197, '1.486339e-04'), (198, '3.114926e-04'), (199, '2.128823e-05'), (200, '1.227051e-04'), (201, '7.311156e-04')]\n",
      "********************************************************************************************************************************\n",
      "backprop! model loss: 1.333811e+00 797.856s| epoch: 90 | grad test: 1.446365e-03,1.446365e-03\n",
      "-------------------UPDATERATIOS_______________________________\n",
      "[(0, '3.273476e-02'), (1, '7.815730e-02'), (2, '2.612979e-02'), (3, '1.708386e-01'), (4, '2.257291e-01'), (5, '2.028138e-01'), (6, '1.877302e-01'), (7, '2.511988e-01'), (8, '1.315705e-01'), (9, '2.650415e-01'), (10, '1.259126e-01'), (11, '1.207752e-01'), (12, '3.557376e-01'), (13, '2.322420e-01'), (14, '1.928331e-01'), (15, '2.204119e-01'), (16, '1.416506e-01'), (17, '1.442527e-01'), (18, '1.589268e-01'), (19, '3.176588e-01'), (20, '1.701027e-01'), (21, '1.938373e-01'), (22, '7.645519e-01'), (23, '4.357936e-02'), (24, '1.018483e-02'), (25, '2.307748e-01'), (26, '1.106573e-01'), (27, '5.759638e-02'), (28, '1.134851e-01'), (29, '7.903049e-02'), (30, '1.460543e-02'), (31, '1.312334e-01'), (32, '1.348094e-01'), (33, '1.205222e-01'), (34, '2.789470e-01'), (35, '1.502780e-01'), (36, '1.147848e-01'), (37, '1.599891e-01'), (38, '1.052366e-01'), (39, '1.710766e-01'), (40, '2.552820e-01'), (41, '1.597959e-01'), (42, '1.441304e-01'), (43, '1.677965e-01'), (44, '1.806215e-01'), (45, '2.000502e-01'), (46, '1.968314e-01'), (47, '1.204910e-01'), (48, '6.893058e-01'), (49, '1.815413e-01'), (50, '3.909678e-01'), (51, '5.540775e-02'), (52, '9.781388e-03'), (53, '1.243070e-01'), (54, '1.459798e-01'), (55, '1.459211e-02'), (56, '1.484715e-01'), (57, '4.851138e-02'), (58, '1.587585e-02'), (59, '2.741639e-01'), (60, '1.215537e-01'), (61, '1.295950e-01'), (62, '1.775055e-01'), (63, '1.118059e-01'), (64, '9.767827e-02'), (65, '1.462439e-01'), (66, '7.875881e-02'), (67, '1.267854e-01'), (68, '1.307279e-01'), (69, '9.903239e-02'), (70, '2.900391e-01'), (71, '1.271249e-01'), (72, '1.296546e-01'), (73, '1.501765e-01'), (74, '9.282862e-02'), (75, '8.495320e-02'), (76, '1.119468e-01'), (77, '1.752456e-01'), (78, '1.467520e-01'), (79, '5.116021e-02'), (80, '9.545949e-03'), (81, '1.727940e-01'), (82, '6.349300e-02'), (83, '7.645207e-03'), (84, '4.606695e-02'), (85, '6.945689e-02'), (86, '1.238001e-02'), (87, '2.192047e-01'), (88, '1.225601e-01'), (89, '1.230498e-01'), (90, '2.019294e-01'), (91, '9.216844e-02'), (92, '1.736685e-01'), (93, '1.881808e-01'), (94, '1.033921e-01'), (95, '1.217771e-01'), (96, '1.261743e-01'), (97, '1.327650e-01'), (98, '1.289732e-01'), (99, '2.009721e-01'), (100, '1.113910e-01'), (101, '1.002423e-01'), (102, '8.676259e-01'), (103, '1.105562e-01'), (104, '1.982266e-01'), (105, '1.928973e-01'), (106, '1.936919e-01'), (107, '1.121285e-01'), (108, '1.091489e-02'), (109, '1.655725e-01'), (110, '1.059509e-01'), (111, '1.023765e-02'), (112, '1.039490e-01'), (113, '1.480547e-01'), (114, '1.122535e-02'), (115, '1.471066e-01'), (116, '1.367864e-01'), (117, '7.927814e-02'), (118, '5.228085e-01'), (119, '1.077062e-01'), (120, '1.248168e-01'), (121, '9.851940e-02'), (122, '1.045030e-01'), (123, '1.219149e-01'), (124, '1.402780e-01'), (125, '1.004263e-01'), (126, '1.486180e-01'), (127, '1.110682e-01'), (128, '1.392542e-01'), (129, '1.356221e-01'), (130, '1.485644e-01'), (131, '1.057963e-01'), (132, '1.005671e-01'), (133, '1.415836e-01'), (134, '1.865608e-01'), (135, '4.912568e-02'), (136, '9.942579e-03'), (137, '1.223636e-01'), (138, '1.154961e-01'), (139, '1.141485e-02'), (140, '7.152605e-02'), (141, '1.044746e-01'), (142, '7.051220e-03'), (143, '3.428520e-01'), (144, '2.642398e-01'), (145, '6.210833e-02'), (146, '9.766309e-02'), (147, '1.231623e-01'), (148, '1.450231e-01'), (149, '1.354948e-01'), (150, '1.172666e-01'), (151, '2.177873e-01'), (152, '1.495816e-01'), (153, '1.702270e-01'), (154, '6.287629e-02'), (155, '1.514259e-01'), (156, '9.590717e-02'), (157, '1.051129e-01'), (158, '8.588844e-02'), (159, '1.373033e-01'), (160, '1.078899e-01'), (161, '1.720403e-01'), (162, '2.023975e-01'), (163, '9.674881e-02'), (164, '7.970893e-03'), (165, '1.651167e-01'), (166, '6.215994e-02'), (167, '5.392004e-02'), (168, '4.118967e-01'), (169, '7.708777e-02'), (170, '5.968532e-03'), (171, '5.725679e-02'), (172, '1.116277e-01'), (173, '1.221741e-01'), (174, '1.706560e-01'), (175, '8.762842e-02'), (176, '3.823427e-01'), (177, '7.842081e-02'), (178, '1.214307e-01'), (179, '6.383984e-02'), (180, '1.166133e-01'), (181, '8.762824e-02'), (182, '9.928322e-02'), (183, '1.837356e-01'), (184, '6.849609e-02'), (185, '1.094328e-01'), (186, '1.176600e-01'), (187, '7.607570e-02'), (188, '5.848989e-02'), (189, '1.169377e-01'), (190, '1.935019e-01'), (191, '8.015407e-02'), (192, '4.046345e-03'), (193, '3.596411e-01'), (194, '2.184877e-01'), (195, '4.576593e-02'), (196, '2.558286e-01'), (197, '1.789450e-01'), (198, '5.104084e-03'), (199, '1.286145e-01'), (200, '6.427989e-02'), (201, '6.608464e-03')]\n",
      "-----------------------------AVGGRADS___________________________________\n",
      "[(0, '4.779303e-05'), (1, '1.937206e-05'), (2, '9.444869e-05'), (3, '2.982225e-05'), (4, '2.597395e-06'), (5, '2.206116e-06'), (6, '1.877741e-05'), (7, '4.091799e-06'), (8, '2.814932e-06'), (9, '1.821892e-05'), (10, '2.978243e-06'), (11, '2.515255e-06'), (12, '1.829557e-05'), (13, '2.367879e-06'), (14, '2.286029e-06'), (15, '1.822464e-05'), (16, '2.943515e-06'), (17, '2.529295e-06'), (18, '1.928700e-05'), (19, '2.091534e-06'), (20, '2.280303e-06'), (21, '1.715545e-05'), (22, '1.929709e-05'), (23, '8.771296e-04'), (24, '1.579912e-04'), (25, '5.134333e-05'), (26, '8.861954e-06'), (27, '9.981630e-05'), (28, '1.388794e-05'), (29, '7.622715e-04'), (30, '1.321729e-04'), (31, '2.892906e-05'), (32, '1.213554e-05'), (33, '9.999874e-06'), (34, '2.232634e-05'), (35, '1.646491e-05'), (36, '1.058516e-05'), (37, '3.446770e-05'), (38, '3.696617e-06'), (39, '2.352913e-06'), (40, '1.881333e-05'), (41, '9.589402e-06'), (42, '7.353556e-06'), (43, '2.332812e-05'), (44, '1.450966e-05'), (45, '1.128192e-05'), (46, '2.686761e-05'), (47, '1.280841e-05'), (48, '1.453725e-05'), (49, '2.770918e-05'), (50, '2.015439e-05'), (51, '6.895038e-04'), (52, '8.973377e-05'), (53, '3.404219e-05'), (54, '3.493106e-06'), (55, '3.261214e-05'), (56, '5.927882e-06'), (57, '6.851706e-04'), (58, '1.602620e-04'), (59, '5.343248e-05'), (60, '1.268191e-05'), (61, '1.281503e-05'), (62, '2.376326e-05'), (63, '1.634688e-05'), (64, '1.501275e-05'), (65, '3.353854e-05'), (66, '1.179420e-05'), (67, '1.922372e-05'), (68, '2.175388e-05'), (69, '1.619073e-05'), (70, '1.602852e-05'), (71, '2.424463e-05'), (72, '1.574939e-05'), (73, '1.358809e-05'), (74, '2.949031e-05'), (75, '8.806664e-06'), (76, '6.322868e-06'), (77, '3.396342e-05'), (78, '2.197698e-05'), (79, '6.568026e-04'), (80, '6.270612e-05'), (81, '3.679309e-05'), (82, '3.488592e-06'), (83, '3.443986e-05'), (84, '4.117157e-06'), (85, '6.633162e-04'), (86, '1.778063e-04'), (87, '6.170992e-05'), (88, '3.565762e-05'), (89, '4.066687e-05'), (90, '4.986843e-05'), (91, '1.651689e-05'), (92, '2.618894e-05'), (93, '2.321244e-05'), (94, '6.837759e-06'), (95, '1.033091e-05'), (96, '3.227033e-05'), (97, '2.598487e-05'), (98, '3.448592e-05'), (99, '4.203443e-05'), (100, '3.507492e-06'), (101, '5.233595e-06'), (102, '6.866715e-06'), (103, '6.221194e-06'), (104, '1.254916e-05'), (105, '8.032914e-06'), (106, '2.163635e-05'), (107, '5.884389e-04'), (108, '5.984047e-05'), (109, '4.829030e-05'), (110, '4.751212e-06'), (111, '3.705566e-05'), (112, '3.973244e-06'), (113, '5.658332e-04'), (114, '2.108167e-04'), (115, '5.359937e-05'), (116, '1.301632e-05'), (117, '2.080222e-05'), (118, '3.355954e-05'), (119, '2.639915e-05'), (120, '4.057920e-05'), (121, '1.442958e-05'), (122, '2.172704e-05'), (123, '2.552497e-05'), (124, '3.451892e-05'), (125, '2.466919e-05'), (126, '3.800074e-05'), (127, '2.532535e-05'), (128, '4.921053e-05'), (129, '4.843531e-05'), (130, '6.338838e-05'), (131, '2.296433e-05'), (132, '2.212624e-05'), (133, '4.326868e-05'), (134, '2.526871e-05'), (135, '4.998541e-04'), (136, '5.386039e-05'), (137, '3.583388e-05'), (138, '4.840067e-06'), (139, '4.070384e-05'), (140, '3.567788e-06'), (141, '5.127131e-04'), (142, '2.665371e-04'), (143, '6.817705e-05'), (144, '3.825307e-05'), (145, '1.737713e-05'), (146, '6.173921e-05'), (147, '3.219373e-05'), (148, '2.023970e-05'), (149, '5.302875e-05'), (150, '3.550143e-05'), (151, '2.067375e-05'), (152, '7.288094e-05'), (153, '4.385852e-05'), (154, '2.212552e-05'), (155, '5.963452e-05'), (156, '3.181628e-05'), (157, '1.710180e-05'), (158, '7.116869e-05'), (159, '3.672664e-05'), (160, '3.268084e-05'), (161, '4.460384e-05'), (162, '2.733289e-05'), (163, '4.329715e-04'), (164, '9.673605e-05'), (165, '4.766420e-05'), (166, '1.284573e-05'), (167, '1.002284e-04'), (168, '8.481449e-06'), (169, '4.131713e-04'), (170, '2.994729e-04'), (171, '1.469789e-04'), (172, '3.285360e-05'), (173, '1.436661e-05'), (174, '7.180929e-05'), (175, '4.277937e-05'), (176, '1.737982e-05'), (177, '6.801875e-05'), (178, '5.125999e-05'), (179, '1.597313e-05'), (180, '8.033596e-05'), (181, '3.786923e-05'), (182, '2.090313e-05'), (183, '9.954301e-05'), (184, '3.933665e-05'), (185, '1.534539e-05'), (186, '8.537605e-05'), (187, '4.486878e-05'), (188, '1.707759e-05'), (189, '8.507753e-05'), (190, '4.452380e-05'), (191, '3.238688e-04'), (192, '1.033863e-04'), (193, '4.903541e-05'), (194, '5.256135e-05'), (195, '2.339645e-04'), (196, '2.153099e-05'), (197, '1.652067e-04'), (198, '3.238400e-04'), (199, '2.400236e-05'), (200, '1.273209e-04'), (201, '7.309658e-04')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     25\u001b[0m a1\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 26\u001b[0m m1\u001b[38;5;241m.\u001b[39mstep(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.5e-3\u001b[39m)\n\u001b[0;32m     28\u001b[0m lri\u001b[38;5;241m.\u001b[39mappend(m1\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\manimtest\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[6], line 470\u001b[0m, in \u001b[0;36mModel.step\u001b[1;34m(self, lr, b1, b2, w, eps)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer\u001b[38;5;241m.\u001b[39mparameters)):\n\u001b[0;32m    469\u001b[0m         layer\u001b[38;5;241m.\u001b[39mmt[i]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m (layer\u001b[38;5;241m.\u001b[39mgrads[i]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mb1) \u001b[38;5;241m+\u001b[39m b1\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mmt[i]\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;66;03m# uncorrected version\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m         layer\u001b[38;5;241m.\u001b[39mvt[i]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m (layer\u001b[38;5;241m.\u001b[39mgrads[i]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mb2) \u001b[38;5;241m+\u001b[39m b2\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mvt[i]\u001b[38;5;241m.\u001b[39mdata) \n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# descending w/ AdamW and weight decay\u001b[39;00m\n\u001b[0;32m    473\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\manimtest\\Lib\\site-packages\\torch\\_tensor.py:35\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[0;32m     33\u001b[0m     assigned \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test-training model!\n",
    "start_time = time.time()\n",
    "print(\"Training,..,.\")\n",
    "\n",
    "for i in range(4500):\n",
    "    x,y = get_batch(batch_size,context_length)\n",
    "    \n",
    "    a = m1(x,y)[0]\n",
    "    a1 = a.clone()\n",
    "        \n",
    "\n",
    "    b = m1.backprop()\n",
    "\n",
    "    stepi.append(sti)\n",
    "    lossi.append(a1.item())\n",
    "    sti += 1\n",
    "\n",
    "    \n",
    "    for layer in m1.layers:\n",
    "        for p in layer.parameters:\n",
    "            p.grad = None\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    a1.backward()\n",
    "    m1.step(lr=4.5e-3)\n",
    "    \n",
    "    lri.append(m1.lr)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"********************************************************************************************************************************\")\n",
    "        vid = torch.randint(20,size=(1,))\n",
    "        print(f\"backprop! model loss: {sum(lossi[-10:])/10:e} {(time.time() - start_time):.3f}s| epoch: {i} | grad test: {m1.layers[2].parameters[0].grad.view(-1)[vid].item():e},{m1.layers[2].grads[0].view(-1)[vid].item():e}\")\n",
    "        print(\"-------------------UPDATERATIOS_______________________________\")\n",
    "        print(list(enumerate(m1.update_ratios)))\n",
    "        print(\"-----------------------------AVGGRADS___________________________________\")\n",
    "        print(list(enumerate(m1.avg_grads)))\n",
    "\n",
    "    \n",
    "\n",
    "for layer in m1.layers:\n",
    "    for i in range(len(layer.parameters)):\n",
    "        # subtracting gradient for each parameter\n",
    "        print(layer.__class__.__name__,i,\" Grad:\",layer.grads[i].view(-1)[10], layer.parameters[i].grad.view(-1)[10])\n",
    "\n",
    "# Problem underlying model\n",
    "# note, final bias should not have gradient...(layer norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a60d2d-900b-4850-b855-2218b186a247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c52f0301d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Why do residual connections and layernormalization work?\n",
    "\n",
    "Layer norm: Even with careful intialization, all gradients are dependent on previous layers. If prev. layers shift in a correlated way\n",
    "(all vals up/down) it will affect the next gradient, causing uneven optimization of layers (all layers can't capture info now). If we normalize\n",
    "the inputs to each layer, this will not happen, and the scale of each layer does not affect the next. But neurons can still have more sensitive outputs\n",
    "this is just learned slowly over time with 'l' and 'b' parameters (scale and shift of neurons' outputs). Gradients are expressive of change,\n",
    "but not sensitive to scale (all grads on similar scale is good).\n",
    "\n",
    "Residual connections: Derivatives of every single layer are 'distributed' through addition, causing each layer, at onset to have similar derivatives.\n",
    "This allows all parts of model to have similar derivatives at start, and then allows for some to take over.\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "What is adamw, and what types of learning rates allow NN's to work best?\n",
    "\n",
    "What is L2 regularization vs. weight decay, how are they 'decoupled'?\n",
    "\n",
    "'''\n",
    "\n",
    "# fix saving function to make it work!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stepi,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c7ea1f1-27bc-4d8f-9472-c58158121587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thePhat! Benk was the or saw a filly had listeng it the was boation and preary. One day, Mach he looked or home and playing a grame liftht hought, Ben offfast tre. The boy had always! He saw ask clied of old forw with made the poce for her, againg shouties worie clossnarts. But the did sad then kyou it? Lily. The twram cam creachaired.\n",
      "Lily it was the friends curcle her friemls. She could play for back and phappy them othe hered. She wanted to little book.\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Once upon a time there was the happy. She loven and hand nothe floress. She hug to ground and scarted and hoppped him in the parent that yileng for this back whald happy sad. He dadlecided to coont the park. The smiled Lily to he cood. She was a prechail annd was that it ell girl named not it was big in the can brabbit the was so specic. She pick and saw that the bear the was with that alway.\n",
      "Liry's pond was proud and t"
     ]
    }
   ],
   "source": [
    "m1.generate(torch.zeros(1,1,device=device,dtype=int),900) # generating bigram characters! - No nonlinearities used, 1 layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0fe6e-372f-498e-9d25-a5e30423be02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
